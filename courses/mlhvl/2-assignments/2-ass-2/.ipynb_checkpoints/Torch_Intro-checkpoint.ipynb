{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üî• Torch Intro üî•\n",
    "\n",
    "## What is PyTorch?\n",
    "PyTorch (or just torch) is a python deep learning library, fresh out of beta but already widely adopted both in research and industry. Like most deep learning libraries it employs GPU accelaration; unlike most deep learning libraries, it supports dynamic computational graphs and deep python integration, enabling easy experimentation and code inspection. It provides high-level abstractions but also allows for low-level access to its primitives. You can read more about PyTorch at the official [site](https://pytorch.org/).\n",
    "\n",
    "## Installing PyTorch on your machine\n",
    "Install PyTorch by following the guidelines here [here](https://pytorch.org/get-started/locally/).\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> If you have an nVidia GPU, installing a CUDA version of PyTorch will allow you to utilize it, significantly speeding up computation.\n",
    "</div>\n",
    "\n",
    "# This Tutorial\n",
    "This tutorial will take you through PyTorch's main functionalities. It only aims to give you some insight on how to use PyTorch and is by no means a full tutorial on neural networks. Prior knowledge of neural netowrks and their inner workings (i.e. linear algebra, gradient-based optimization, back-propagation etc) will certainly prove useful. You are assumed to know python fairly well.\n",
    "\n",
    "# Table of Contents\n",
    "1. [Tensors](#1)\n",
    "    1. [Tensor Types](#1a)\n",
    "    2. [Instantiating Tensors](#1b)\n",
    "    3. [Basic Tensor Operations](#1c)\n",
    "    4. [Exercises](#1d)\n",
    "2. [Automatic Differentation](#2)\n",
    "    1. [Autograd](#2a)\n",
    "    2. [Exercises](#2b)\n",
    "3. [Neural Networks](#3)\n",
    "    1. [Custom Neural Networks](#3a)\n",
    "    2. [Loss Functions](#3b)\n",
    "    3. [Optimizers](#3c)\n",
    "    4. [Exercises](#3d)\n",
    "4. [Putting Everything Together](#4)\n",
    "\n",
    "\n",
    "For a more in-depth overview of PyTorch's capabilities, refer to the [official documentation](https://pytorch.org/docs/stable/index.html) (this link will prove handy for your assignments-- keep it close and use it often).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting started\n",
    "Let's verify your torch installation is working by trying to import it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1'></a>\n",
    "## 1. Tensors\n",
    "\n",
    "A [Tensor](https://pytorch.org/docs/stable/tensors.html) is the building block of any PyTorch program; it is the abstraction that stores n-ary arrays of numbers (i.e. tensors) and provides various functionalities for processing them. \n",
    "\n",
    "<a id='1a'></a>\n",
    "### A. Tensor Types\n",
    "\n",
    "There are 16 types of Tensors, distinguished by their `dtypes` (the sort of numbers stored within them) and the `device` they can be accessed by (either GPU or CPU).\n",
    "\n",
    "The different Tensor types and their corresponding classes are shown below:\n",
    "\n",
    "|  | dtype | CPU Tensor Class | GPU Tensor Class |\n",
    "| --- | --- | --- | --- |\n",
    "| Full precision float | `torch.float32` | `torch.FloatTensor` | `torch.cuda.FloatTensor`| \n",
    "| Half precision float | `torch.float16` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |\n",
    "| Double precision float | `torch.float64` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |\n",
    "| 8-bit unsigned integer | `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor` |\n",
    "| 8-bit signed integer | `torch.int8` | `torch.CharTensor` | `torch.CharTensor` |\n",
    "| 16-bit signed integer | `torch.int16` | `torch.ShortTensor` | `torch.cuda.ShortTensor` |\n",
    "| 32-bit signed integer | `torch.int32` | `torch.IntTensor` | `torch.cuda.IntTensor` |\n",
    "| 64-bit signed integer | `torch.int64` | `torch.LongTensor` | `torch.cuda.LongTensor` |\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Interaction between Tensors of different devices or dtypes is not permitted (so make sure you are consistent). \n",
    "</div>\n",
    "\n",
    "We are mostly interested in full precision floats and long integers (on either device), so we can forget about the rest of them for now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1b'></a>\n",
    "### B. Instantiating Tensors\n",
    "Tensors can be instantiated in a number of ways. When we want to construct a placeholder tensor of fixed dimensionality, we may simply call the appropriate class constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_first_long_tensor = torch.LongTensor(5)  # a vector of 5 longs\n",
    "my_first_float_tensor = torch.FloatTensor(5,5)  # a 5 by 5 matrix of floats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we may use the abstract constructor `torch.tensor` and use the `dtype` and `device` arguments to specify its type (these default to torch.float and cpu, in the general case)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_second_long_tensor = torch.tensor(5, device='cpu', dtype=torch.long) # another vector of 5 longs\n",
    "my_second_float_tensor = torch.tensor((5, 5), device='cpu', dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the sake of convenience, we will now specify the device used by the rest of the tutorial. If you have the cuda version installed but would rather not use it, change the snippet below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    device = 'cpu'\n",
    "\n",
    "print('Using {}'.format(device))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a few useful short-hands for constructing tensors with commonly used values. Let's use some of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((2,3,4), device=device)  # a 2 by 3 by 4 tensor of zeroes\n",
    "b = torch.ones(42, device=device)  # a vector of 42 ones\n",
    "c = torch.eye(3, device=device)  # the 3 by 3 identity matrix\n",
    "d = torch.rand((32,10,300), device=device)  # a 32 by 10 by 300 tensor of randoms\n",
    "e = torch.randint(low=0, high=10, size=(3,3), device=device)  # a 3 by 3 matrix of random integers between 0 and 10\n",
    "f = torch.arange(10, device=device)  # a vector containing the numbers 0 to 9 in ascending order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can always query a tensor's contents, shape, dtype and device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "cpu\n",
      "\n",
      "\n",
      "torch.Size([42])\n",
      "cpu\n",
      "\n",
      "\n",
      "torch.Size([3, 3])\n",
      "cpu\n",
      "\n",
      "\n",
      "torch.Size([32, 10, 300])\n",
      "cpu\n",
      "\n",
      "\n",
      "torch.Size([3, 3])\n",
      "cpu\n",
      "\n",
      "\n",
      "torch.Size([10])\n",
      "cpu\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for te in [a, b, c, d, e, f]:\n",
    "    print(te.shape)\n",
    "    print(te.device)\n",
    "    # print(te)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the values of a tensor by passing a list (of lists*) of values during its construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]])\n",
      "torch.Size([3, 3])\n",
      "torch.int64\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1,2,3], [4,5,6], [7,8,9]], device=device)\n",
    "print(a)\n",
    "print(a.shape)\n",
    "print(a.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that PyTorch automatically assumed that the tensor we specified should be of type long (because we only provided integers as the tensor's contents). We could of course avoid this by manually specifying the dtype. Alternatively, we can alter the dtype and/or device post-construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "cpu\n"
     ]
    }
   ],
   "source": [
    "a = a.to(torch.float)\n",
    "print(a.dtype)\n",
    "a = a.to('cpu')  # or alternatively, a = a.cpu()\n",
    "print(a.device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, a torch tensor can also be directy constructed by (or converted to) a numpy array. Converting to a numpy array only works for _cpu_ tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4125857  0.35930604]\n",
      " [0.39403742 0.06137439]]\n",
      "tensor([[0.4126, 0.3593],\n",
      "        [0.3940, 0.0614]], dtype=torch.float64)\n",
      "[[0.4125857  0.35930604]\n",
      " [0.39403742 0.06137439]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "a_np = np.random.random((2,2))\n",
    "a_torch = torch.tensor(a_np, device=device)\n",
    "print(a_np)\n",
    "print(a_torch)\n",
    "a_np_2 = a_torch.cpu().numpy()\n",
    "print(a_np_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a_np, a_torch, a_np_2, b, c, d, my_first_long_tensor, my_first_float_tensor, my_second_long_tensor, my_second_float_tensor, e, f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1c'></a>\n",
    "### C. Basic Tensor Operations\n",
    "Tensors and their contents' are not hidden by the framework-- they are immediatelly accessible to us and we can interact with them in many ways, while being able to inspect the results of our actions. Let's walk through some of the most common usecases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Indexing and Slicing\n",
    "Standard python indexing and slicing applies to torch tensors. Let's remember how that works-- first we will need a random matrix to experiment with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8386, 0.7454, 0.3470],\n",
      "        [0.9717, 0.3686, 0.1437],\n",
      "        [0.4932, 0.8564, 0.0325],\n",
      "        [0.0975, 0.0705, 0.7053],\n",
      "        [0.9187, 0.3810, 0.1042]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.rand((5,3), device=device)  \n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Remember!</b>\n",
    "Indexing starts from zero\n",
    "</div>\n",
    "\n",
    "Now let's try retrieving the 3rd item of the 1st row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.3470)\n"
     ]
    }
   ],
   "source": [
    "b = a[0][2]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we wanted the first three rows of the matrix instead?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.8386, 0.7454, 0.3470],\n",
      "        [0.9717, 0.3686, 0.1437],\n",
      "        [0.4932, 0.8564, 0.0325]])\n"
     ]
    }
   ],
   "source": [
    "c = a[:3]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or its third column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3470, 0.1437, 0.0325, 0.7053, 0.1042])\n"
     ]
    }
   ],
   "source": [
    "c = a[:, 2]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or every second element of the first column, starting from the second?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9717, 0.0975])\n"
     ]
    }
   ],
   "source": [
    "c = a[1::2, 0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But now in reverse, starting from the last?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "negative step not yet supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-239-750cb89b0dfc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: negative step not yet supported"
     ]
    }
   ],
   "source": [
    "c = a[-1::-2, 0]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, perhaps not.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Value Assignment\n",
    "We can use the exact same scheme to assign values to tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0000, 0.7454, 0.3470],\n",
      "        [0.9717, 0.3686, 0.1437],\n",
      "        [0.4932, 0.8564, 0.0325],\n",
      "        [0.0975, 0.0705, 0.7053],\n",
      "        [0.9187, 0.3810, 0.1042]])\n",
      "tensor([[0.3704, 0.9572, 0.8403],\n",
      "        [0.1303, 0.5892, 0.0101],\n",
      "        [0.0212, 0.3841, 0.6547],\n",
      "        [0.4819, 0.6911, 0.9733],\n",
      "        [0.7575, 0.0748, 0.9876]])\n",
      "tensor([[0.0000, 0.7454, 0.3470],\n",
      "        [0.0212, 0.3841, 0.6547],\n",
      "        [0.4932, 0.8564, 0.0325],\n",
      "        [0.0975, 0.0705, 0.7053],\n",
      "        [0.9187, 0.3810, 0.1042]])\n"
     ]
    }
   ],
   "source": [
    "# Set the top left item of the matrix to zero.\n",
    "a[0,0] = 0\n",
    "print(a)\n",
    "\n",
    "# Construct another random matrix of the same shape\n",
    "b = torch.rand_like(a, device=device)\n",
    "print(b)\n",
    "\n",
    "# Set the second row of matrix a to be the third row of matrix b\n",
    "a[1] = b[2]\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Element-wise Arithmetic\n",
    "Element-wise operations (most importantly comparison, addition, subtraction, multiplication and division) can be applied on tensors of compatible shapes (i.e. shapes that can be [broadcasted](https://pytorch.org/docs/stable/notes/broadcasting.html)). \n",
    "\n",
    "Two tensors are compatible if any of the two below conditions hold:\n",
    "* their shapes are the same \n",
    "* their trailing (i.e. last) N dimensions are the same (possibly excluding missing dimensions and dimensions of size 1)\n",
    "\n",
    "Scalars (single values) are compatible with tensors of any shape. Let's see some examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, some fresh tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.zeros((2, 3, 4), device=device)  \n",
    "b = torch.ones((2, 3, 4), device=device)  \n",
    "c = torch.ones((3, 4), device=device)  \n",
    "d = torch.ones((2, 3, 1), device=device)\n",
    "e = torch.rand((4, 3, 2), device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add a scalar to tensor a "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.],\n",
      "         [0., 0., 0., 0.]]])\n",
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "a = a + 0.3\n",
    "print(a.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can subtract b from a  (matching shapes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[-0.7000, -0.7000, -0.7000, -0.7000],\n",
      "         [-0.7000, -0.7000, -0.7000, -0.7000],\n",
      "         [-0.7000, -0.7000, -0.7000, -0.7000]],\n",
      "\n",
      "        [[-0.7000, -0.7000, -0.7000, -0.7000],\n",
      "         [-0.7000, -0.7000, -0.7000, -0.7000],\n",
      "         [-0.7000, -0.7000, -0.7000, -0.7000]]])\n"
     ]
    }
   ],
   "source": [
    "f = a - b\n",
    "print(f.shape)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can elementwise multiply a with c  (dimensions of c are the same as last dimensions of a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[0.3000, 0.3000, 0.3000, 0.3000],\n",
      "         [0.3000, 0.3000, 0.3000, 0.3000],\n",
      "         [0.3000, 0.3000, 0.3000, 0.3000]],\n",
      "\n",
      "        [[0.3000, 0.3000, 0.3000, 0.3000],\n",
      "         [0.3000, 0.3000, 0.3000, 0.3000],\n",
      "         [0.3000, 0.3000, 0.3000, 0.3000]]])\n"
     ]
    }
   ],
   "source": [
    "g =  a * c\n",
    "print(g.shape)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can elementwise divide a by d  (last dimension is 1, the rest of the dimensions match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "h = a / d\n",
    "print(h.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can elementwise raise a to d  (last dimension of d is 1, the rest of the dimensions match)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n"
     ]
    }
   ],
   "source": [
    "i = a**d\n",
    "print(i.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can compare a with any of f, g, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4])\n",
      "tensor([[[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]],\n",
      "\n",
      "        [[False, False, False, False],\n",
      "         [False, False, False, False],\n",
      "         [False, False, False, False]]])\n"
     ]
    }
   ],
   "source": [
    "j = a == f\n",
    "print(j.shape)\n",
    "print(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. but torch complains when we try to do that with e (shapes are incompatible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-248-a2c3e750b318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0me\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m: The size of tensor a (2) must match the size of tensor b (4) at non-singleton dimension 2"
     ]
    }
   ],
   "source": [
    "e == a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "del a, b, c, d, e, f, g, h, i, j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If in doubt for what any of the element-wise operators actually do, try them out below on some tensors of your own making."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1.],\n",
      "         [1., 1., 1.],\n",
      "         [1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones((2,3,3), device=device)\n",
    "print(a)\n",
    "del(a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Linear Algebra\n",
    "Tensor algebra of course goes well beyond elementwise operations- matrix multiplication is the bread and butter of machine learning, so we better get familiar with how torch does it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As usual, we begin by instantiating our matrices. Matrix multiplication is defined between matrices A and B of shapes [M, N] and [N, O] respectively and yields a matrix C of shape [M, O]. The torch function that implements matrix multiplication is `torch.mm`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 8])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([5,3], device=device)\n",
    "B = torch.rand([3,8], device=device)\n",
    "C = torch.mm(A, B)  # or alternatively, C = A @ B\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5, 8])\n"
     ]
    }
   ],
   "source": [
    "bA = torch.rand([128, 5, 3], device=device)\n",
    "bB = torch.rand([128, 3, 8], device=device)\n",
    "bC = torch.bmm(bA, bB)  # bC = bA @ bC also works here!\n",
    "print(bC.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Be careful not to confuse matrix multiplication `A@B` with the [Hadamard product](https://en.wikipedia.org/wiki/Hadamard_product_%28matrices%29) `A*B`\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Shape Manipulation\n",
    "As we have seen, what we can do with tensors is largely dictated by their shapes. Adjusting a tensor's shape to allow for broadcasting or batching is therefore often necessary. The following functions should suffice for the bulk of shape manipulation tasks you might encounter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensor tansposition is the generalization of matrix transposition. Since there are now more than 2 dimensions, we additionally need to specify the transposed dimensions. Take for instance a tensor of shape [M, N, O]. Converting it to a tensor of shape [N, M, O] requires transposing the first and second dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 128, 3])\n",
      "torch.Size([5, 3, 128])\n",
      "torch.Size([3, 5, 128])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([128, 5, 3], device=device)\n",
    "A = A.transpose(0, 1)\n",
    "print(A.shape)\n",
    "A = A.transpose(1,2)\n",
    "print(A.shape)\n",
    "A = A.transpose(0,1)\n",
    "print(A.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may also choose to create a reshaped view of a tensor; for instance we may collapse two or more tensor dimensions into one.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([640, 3])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([128, 5, 3], device=device)\n",
    "A_collapsed = A.view(A.shape[0]*A.shape[1], A.shape[-1])\n",
    "print(A_collapsed.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    ".. or expand one dimension into two or more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([128, 5, 3])\n"
     ]
    }
   ],
   "source": [
    "A_expanded = A_collapsed.view(128, 5, 3)\n",
    "print(A_expanded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convince ourselves that the back and forth between dimensions has left our tensor unaffected. First let's elementwise compare A with A_expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True]],\n",
      "\n",
      "        [[True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True]],\n",
      "\n",
      "        [[True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True]],\n",
      "\n",
      "        [[True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True]],\n",
      "\n",
      "        [[True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True],\n",
      "         [True, True, True]]])\n"
     ]
    }
   ],
   "source": [
    "comp = A == A_expanded\n",
    "print(comp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems to be okay! But what if there is a zero somewhere in there? Python's `all` and `any` may be used directly on torch bools to help us here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print(all([all(row) for matrix in comp for row in matrix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Views are useful, but as the name suggests they only change our view of a tensor. Different views of a tensor have the same number of elements; the view is just changing in what order these are read.\n",
    "\n",
    "For cases where we would like to repeat a tensor across one or more of its axes (actually creating a larger tensor), we can use the function of the same name. Let's consider a tensor of shape [M, N] which we would like to turn into a tensor that repeats itself K times across the first dimension (i.e. a tensor of shape [K $\\cdot$ M, N]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([15, 12])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([5, 12], device=device)\n",
    "A_repeat = A.repeat(3, 1)  # note that we are specifying the number of repeats per dimension\n",
    "print(A_repeat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Combining Tensors\n",
    "Sometimes we may want to construct a big tensor out of two small ones. There's a few ways to accomplish that, but the most reliable one is through `torch.cat` üêà (shorthand for concatenate).\n",
    "\n",
    "Two tensors may be concatenated if they agree on all their dimensions, except for the concatenation dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 2])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand([4,2], device=device)\n",
    "B = torch.rand([1,2], device=device)\n",
    "C = torch.cat((A, B), dim=0)\n",
    "print(C.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A, B, C, comp, A_repeat, A_expanded, A_collapsed, bA, bB, bC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='1d'></a>\n",
    "### Exercises\n",
    "It might be a good idea to take a short break here and recap on what we've seen before moving further. The mini-exercises below should help you test your grasp of this section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct a tensor $A$ of shape [10, 10] containing random floats, and a tensor $B$ of the same shape where all its elements are equal to $\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand((10,10), device=device)\n",
    "print(A.shape)\n",
    "B = torch.zeros((10,10), device=device)\n",
    "B = B.new_full((10,10), 3.14, device=device)\n",
    "print(B.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute $C = AB^T$, the matrix multiplication of $A$ with the transpose of $B$ and $D = A\\cdot B$, their elementwise multiplication."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 10])\n",
      "torch.Size([10, 10])\n"
     ]
    }
   ],
   "source": [
    "C = torch.mm(A,B.transpose(0,1))\n",
    "print(C.shape)\n",
    "D = A*B\n",
    "print(D.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try comparing $C$ with $D$. Are they comparable? Are they equal? What is the dtype of their comparison?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]\n",
      " [False False False False False False False False False False]]\n",
      "tensor([[False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False],\n",
      "        [False, False, False, False, False, False, False, False, False, False]])\n",
      "torch.bool\n"
     ]
    }
   ],
   "source": [
    "# First way to compare\n",
    "f = C == D\n",
    "print(f.numpy().astype('bool'))\n",
    "# Second way to compare\n",
    "print(torch.eq(C, D))\n",
    "# Print dtype of comparison\n",
    "print(f.dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dtype of their comparison is a torch Boolean.\n",
    "They are not equal but they are comparable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiply $A$ by 4 to create $F$. Now set all elements of $F$ that are above $\\pi$ to zero.\n",
    "\n",
    "\n",
    "_Hint 1_: You can index a tensor with a boolean tensor of the same dimensionality\n",
    "\n",
    "_Hint 2_: You can set multiple indexed elements to a single value at once"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "F = A*4\n",
    "F[torch.gt(F, 3.14)] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The incomplete function below implements matrix multiplication with a for loop. Complete the function and call it with your $A$ and $B^T$ matrices as its arguments. The result should be the same as the matrix $C$ you computed before (with room for some numerical inaccuraccy)\n",
    "\n",
    "Note: You can use `torch.sum()` to compute the sum of a tensor (optionally specifying across which dimension)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> [Type Hints](https://www.python.org/dev/peps/pep-0484/#rationale-and-goals) may be used in python function and variable declarations to give them a type signature. These type signatures are not strict (you can still bypass them), but they can help you organize your code. Type hints of incomplete functions given during assignments will inform you of what we expect your function to accept and return.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_mm(A: torch.FloatTensor, B: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    assert A.shape[1] == B.shape[0]\n",
    "    assert (len(A.shape) == len(B.shape) == 2)\n",
    "    C = torch.zeros((A.shape[0], B.shape[1]), device=device)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            C[i, j] = torch.sum(A[i,]*B[:,j])\n",
    "    return C\n",
    "    \n",
    "E = my_mm(A, B.transpose(1,0))\n",
    "\n",
    "# make sure the mean absolute difference between the two results is below 0.0001\n",
    "assert torch.sum(torch.abs(E - C))/(E.shape[0]*E.shape[1]) < 1e-4 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this I used the Hadamard product, given that the tasks did not specify particular constraints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a 2-dimensional tensor $S$ that consists by an instance of $A$ followed by two instances of $B$ followed by an instance of $A$ across its first dimension. What is the shape of $S$?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([40, 10])\n"
     ]
    }
   ],
   "source": [
    "S = torch.cat((A,B,B,A), dim=0)\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape $S$ into a tensor of shape [20, 2, 10]. \n",
    "Then transpose this into a tensor of shape [2, 10, 20]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 10, 20])\n"
     ]
    }
   ],
   "source": [
    "S_expanded = S.view(20, 2, 10)\n",
    "S = S_expanded.transpose(0,1)\n",
    "S = S.transpose(1,2)\n",
    "print(S.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A, B, C, D, E, F, S"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2'></a>\n",
    "## 2. Automatic Differentation and Neural Networks\n",
    "We have so far seen some of torch's computational capabilities; its GPU accelaration and multitude of high level functions make it suitable for array processing and vector arithmetic. But torch is more than a faster numpy; its key components, and where most of the magic happens, are in its automatic differentiation mechanics and neural network libraries.\n",
    "\n",
    "<a id='2a'></a>\n",
    "### A. Autograd\n",
    "Each torch tensor carries a flag around with it, `requires_grad`, which establishes whether that tensor requires gradient computation. \n",
    "\n",
    "By default, tensors do not require grad unless specified to. Whenever a tensor that requires grad assumes a role in the construction of another tensor, the new tensor also requires grad.\n",
    "By dynamically tracking dependencies in the evolving computation graph, and utilizing this flag, torch is able to inform itself on which tensors need to be updated by gradient descent, and which do not (naturally, only tensors for which gradients are computed will be updated).\n",
    "\n",
    "Practically, by setting `requires_grad` to `False` we can _freeze_ (parts of) our functions, making them static.\n",
    "\n",
    "Let's see this in action by modeling a simple linear transformation $f(x): Ax$ from $x \\in \\mathbb{R}^5$  to $y \\in \\mathbb{R}^7$:\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Recall that a matrix of shape [M, N] is a linear map __from__ $\\mathbb{R}^N$ __to__ $\\mathbb{R}^M$.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((7, 5), device=device) \n",
    "\n",
    "def f(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return A@x "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test our function for some $x$, and check whether the result requires grad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3738, 0.6592, 0.5001, 0.3708, 0.4709, 0.4204, 0.6495])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(5, device=device)\n",
    "y = f(x)\n",
    "print(y)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does not; what if the parameters of our function $f$ were trainable though?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "A.requires_grad = True\n",
    "y = f(x)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of our linear transformation is now also trainable!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='2b'></a>\n",
    "### Exercises\n",
    "Answer the next questions before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model an affine transformation $g(x) = Ax + \\beta$ from $x \\in \\mathbb{R}^3$ to $y \\in \\mathbb{R}^{12}$ as the composition of two functions, $f_1(x) = Ax$, $f_2(x) = x + \\beta$, such that $A$ requires grad but $\\beta$ does not.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> You can use `requires_grad: bool` as an optional argument during tensor construction. Can you guess its default value?\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5839, 0.4516, 0.9754])\n"
     ]
    }
   ],
   "source": [
    "x = torch.rand(3, device=device)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = torch.rand((12, 3), requires_grad=True,  device=device) \n",
    "beta = torch.rand(12, device=device)\n",
    "\n",
    "def f_1(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return A@x\n",
    "\n",
    "def f_2(x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "    return x + beta\n",
    "\n",
    "x = torch.rand(3, device=device)\n",
    "\n",
    "w = f_1(x)\n",
    "y = f_2(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to figure out the answers on your own before checking the code.\n",
    "\n",
    "Let's assume $x$ is a fixed data sample, therefore does not require grad (we don't usually want to fit our data, but the function applied on the data!)\n",
    "\n",
    "* If $A$ requires grad but $\\beta$ doesn't, does $w$ require grad? Does $y$?\n",
    "\n",
    "* If $\\beta$ requires grad but $A$ doesn't, does $w$ require grad? Does $y$?\n",
    "\n",
    "What would that mean for $A$ and $\\beta$ during gradient descent?\n",
    "\n",
    "Verify your answers with code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand((12, 3), requires_grad=True,  device=device) \n",
    "beta = torch.rand(12, requires_grad=False, device=device)\n",
    "\n",
    "w = f_1(x)\n",
    "y = f_2(w)\n",
    "\n",
    "print(w.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "A = torch.rand((12, 3), requires_grad=False,  device=device) \n",
    "beta = torch.rand(12, requires_grad=True, device=device)\n",
    "\n",
    "w = f_1(x)\n",
    "y = f_2(w)\n",
    "\n",
    "print(w.requires_grad)\n",
    "print(y.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "metadata": {},
   "outputs": [],
   "source": [
    "del A, beta, x, y, w, f, f_1, f_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3'></a>\n",
    "\n",
    "## 3. Neural Networks\n",
    "We are very close to defining our first neural network in torch. Torch includes a powerful neural network library, `torch.nn`, which allows us to create our own custom network flows, use highly optimized off-the-shelf implementations of most standard kinds of networks and compose different networks together.\n",
    "\n",
    "<a id='3a'></a>\n",
    "### A. Custom Neural Networks\n",
    "You have already implemented an affine transformation; a shallow feedforward network is simply such a transformation followed by a non-linearity. For the sake of familiarizing ourselves with custom torch networks, we will go through the process of defining such a network from scratch (you won't normally be doing this, but it's still beneficial to have an idea of what's happening at the low level before proceeding to the high level).\n",
    "\n",
    "The building block for a torch network is the `torch.nn.Module` class; we need to define our networks as objects inheriting that class. If we do so, we only need to implement two functions: `__init__()` and `forward()`. The first is responsible for registering the internal variables of our network, while the second specifies the kind of computation it actually performs.\n",
    "\n",
    "Let's see these in practice; we will define a shallow feedforward network implementing $f(x) = \\sigma(Wx + \\beta)$ from any input dimension to any output dimension, where $\\sigma$ the sigmoid activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_first_network(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, out_features: int, device: str) -> None:\n",
    "        super(my_first_network, self).__init__()  # this is important! do not forget to call this\n",
    "        self.device = device\n",
    "        self.W = torch.nn.Parameter(torch.rand(out_features, in_features, device=self.device))\n",
    "        self.beta = torch.nn.Parameter(torch.rand(out_features, device=self.device))\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return torch.sigmoid(self.W@x + self.beta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Notice the use of torch.nn.Parameter. Wrapping the tensors that are parametric to our network's function in torch.nn.Parameter is crucial as it informs torch that these tensors need to be stored, updated and shared between different function calls.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now test our first network class by instatiating an actual network and passing a random tensor through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5640, 0.7497, 0.6879, 0.7985, 0.7018, 0.7336, 0.7202, 0.7830, 0.7601,\n",
      "        0.5946, 0.7007, 0.6208], grad_fn=<SigmoidBackward>)\n",
      "tensor([0.1203, 0.0962, 0.3655])\n"
     ]
    }
   ],
   "source": [
    "f = my_first_network(in_features=3, out_features=12, device=device)\n",
    "x = torch.rand(3, device=device)\n",
    "y = f(x)\n",
    "print(y)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Notice that we can use `f(x)` instead of `f.forward(x)` -- `forward` overloads the `__call__` method of the `torch.nn.Module` class, so the above statements are equivalent.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have the abstraction for one layer, what's stopping us from instantiating a second network and composing the two into a deep network?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.9830, 0.9800, 0.9955, 0.9933, 0.9839], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "class my_first_deep_network(torch.nn.Module):\n",
    "    def __init__(self, in_features: int, intermediate_features: int, out_features: int, device: str) -> None:\n",
    "        super(my_first_deep_network, self).__init__()\n",
    "        self.device = device\n",
    "        self.n_1 = my_first_network(in_features=in_features, out_features=intermediate_features, device=self.device)\n",
    "        self.n_2 = my_first_network(in_features=intermediate_features, out_features=out_features, device=self.device)\n",
    "        \n",
    "    def forward(self, x: torch.FloatTensor) -> torch.FloatTensor:\n",
    "        return self.n_2(self.n_1(x))\n",
    "    \n",
    "g = my_first_deep_network(in_features=3, intermediate_features=12, out_features=5, device=device)\n",
    "y = g(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very easy, right?\n",
    "\n",
    "Except we just did it the hard way! We could have used torch's pre-made `torch.nn.Linear`, the existing abstraction for single feedforward layers, and `torch.nn.Sequential`, the abstraction for composing sequences of networks. `Sequential` is initiated by an iterable of neural modules (not functions!), which are applied in the order specified.\n",
    "\n",
    "How would that have looked like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.3952, 0.5643, 0.5689, 0.5022, 0.5784], grad_fn=<SigmoidBackward>)\n"
     ]
    }
   ],
   "source": [
    "h = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=3, out_features=12),\n",
    "    torch.nn.Sigmoid(), \n",
    "    torch.nn.Linear(in_features=12, out_features=5), \n",
    "    torch.nn.Sigmoid()  \n",
    ").to(device) \n",
    "\n",
    "y = h(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even easier!\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> When composing networks using the `Sequential` abstraction, you should make sure that each network's expected input shape matches the output shape of the previous network. The device conversion is applied recursively to each sub-module within a module, ensuring that all components of the network live happily in the same device. üè† \n",
    "</div>\n",
    "\n",
    "So, why ever define our own networks if torch can do it for us? The answer is that very often (and very soon!) it might be the case that you'll need to write your own, potentially complex, computation flow, which won't necesserily be possible to rephrase as simple layer stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "del x, y, h, g, f, my_first_network, my_first_deep_network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3b'></a>\n",
    "### B.  Loss Functions\n",
    "\n",
    "We have seen how to construct parametric, trainable functions and networks. Requiring a gradient and having a gradient are two different things, however. To obtain the gradients of our trainable parameters we need a loss function. The loss function is an indicator of how far off the network's output (prediction) is from the actual truth. Applying the chain rule, we may differentiate the loss value w.r.t. the model's parameters, populating the tensors' gradients in the process. \n",
    "\n",
    "As with networks, torch provides implementations for the commonly used loss functions but also allows us to write our own (as another neural module!).\n",
    "\n",
    "We will only experiment with an existing loss function, but first we need to construct some data to play with-- we can create a synthetic data set of pairs $(x_i, y_i)$ where $x_i$ is a random number and $y_i = 3 \\cdot x_i - 2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a tensor of shape 100, 1, i.e. 100 data points each of dimensionality 1\n",
    "x = torch.rand((100, 1), device=device) \n",
    "y = 3 * x - 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Time to employ a linear network.\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "Don't mix up input/output dimensionality and number of data samples! \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.nn.Linear(in_features=1, out_features=1).to(device)\n",
    "prediction = f(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define our loss function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that each element of our batch (i.e. each of the 100 data samples) has its own MSE w.r.t. to its corresponding output. These unique losses are averaged into a single scalar by the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(1.9035, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "loss = loss_fn(y, prediction)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have computed the loss, we may use it to populate the parameters' gradients via a backward pass. This is automagically done by a simple call of `backward`. üßô"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Torch provides a few different loss functions, each for a particular usecase (the task and your output layer's activation function). Note that particular loss functions are already implementing the network's output activation internally. Refer to the [documentation](https://pytorch.org/docs/stable/nn.html#loss-functions) for a detailed overview. In most cases, the cheat-sheet below should contain the answer.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Tip:</b> At a loss on which loss to use? Use this cheat-sheet!\n",
    "</div>\n",
    "\n",
    "| Task | Activation | Loss Function |\n",
    "| --- | --- | --- | --- |\n",
    "| K-class Classification | - | CrossEntropyLoss |\n",
    "| K-class Classification | LogSoftmax | NLLLoss |\n",
    "| K-class, Multi-Label Classification | - | BCEWithLogitsLoss |\n",
    "| K-class, Multi-Label Classification | LogSigmoid | BCELoss |\n",
    "| Continuous Regression | - | MSELoss |\n",
    "| Probability Distribution Fitting | LogSoftmax / LogSigmoid | KLDivLoss |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3c'></a>\n",
    "\n",
    "### C.  Optimizers\n",
    "Our struggles are slowly coming to an end. We have made a trainable network, we have computed the loss given the true output, and we have used the loss to populate the parameter gradients. The final thing to do is to use these gradients in order to update the parameter values. \n",
    "This is managed by an `Optimizer`. Gradient based optimizers are the norm for training neural networks; all of them are variants of stochastic gradient descent. Torch provides implementations of the classic optimizers. Regardless of which one of them is your favourite, the process always involves the same steps:\n",
    "1. Initiate the optimizer by letting it know which parameters it is going to be responsible for\n",
    "2. Iterate over your data, and:\n",
    "    2. Compute the loss\n",
    "    3. Back-propagate\n",
    "    4. Perform an optimization step\n",
    "    5. Zero out the gradients (so that they don't accumulate over optimization steps)\n",
    "\n",
    "Let's see them in action on our toy network and synthetic dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 1.9035359621047974\n",
      "Iteration 500 loss: 1.3216577768325806\n",
      "Iteration 1000 loss: 0.8840070962905884\n",
      "Iteration 1500 loss: 0.564766526222229\n",
      "Iteration 2000 loss: 0.3399919867515564\n",
      "Iteration 2500 loss: 0.18917618691921234\n",
      "Iteration 3000 loss: 0.09469819813966751\n",
      "Iteration 3500 loss: 0.04106728732585907\n",
      "Iteration 4000 loss: 0.014641785062849522\n",
      "Iteration 4500 loss: 0.003998484928160906\n",
      "Iteration 5000 loss: 0.0007626206497661769\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(f.parameters())  # initiate optimizer\n",
    "for t in range(5001):  # iterate\n",
    "    prediction = f(x)  # predict \n",
    "    loss = loss_fn(prediction, y)  # compute loss\n",
    "    loss.backward()  # backpropagate\n",
    "    opt.step()  # optimize\n",
    "    opt.zero_grad()  # reset gradients\n",
    "    if t%500 == 0:\n",
    "        print('Iteration {} loss: {}'.format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have trained your first torch network! üéâ\n",
    "\n",
    "You can now probe its inner parameters to see to what extent it figured out the truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[2.9126]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([-1.9462], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "print(f.weight)\n",
    "print(f.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3d'></a>\n",
    "\n",
    "### D. Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we move on to a somewhat more realistic problem, it might be a good idea to get further acquainted with the basics. \n",
    "\n",
    "Let's take a quick look at a few different non-linear activations first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `arange` to construct a float tensor $x$ of values $0 \\dots 1000$ in ascending order. Then element-wise subtract $500$ and divide by $100$ to get a tensor of values $-5 \\dots 5$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(0,1000, dtype=torch.float64 , device=device)\n",
    "#print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construct the tensors $s = \\sigma(x)$, $t = tanh(x)$ and $r = ReLU(x)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.sigmoid(x)\n",
    "t = torch.tanh(x)\n",
    "r = torch.relu(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can convert these to numpy arrays and plot the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(x.cpu().numpy(),s.cpu().numpy())\n",
    "plt.plot(x.cpu().numpy(),t.cpu().numpy())\n",
    "plt.plot(x.cpu().numpy(),r.cpu().numpy())\n",
    "plt.ylim((-2, 5))\n",
    "plt.legend(['sigmoid', 'tanh', 'relu'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's practice some more by solving the infamous XOR problem with a small deep network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0., 1., 1., 0.])\n",
      "tensor([[0.],\n",
      "        [1.],\n",
      "        [1.],\n",
      "        [0.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.tensor([[0, 0], [0, 1], [1, 0], [1, 1]], device=device, dtype=torch.float)\n",
    "Y = torch.tensor([0, 1, 1, 0], device=device, dtype=torch.float)\n",
    "Y1 = torch.tensor([0, 1, 1, 0], device=device, dtype=torch.float).view(-1,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use `torch.nn.Sequential` to create a minimal deep network with 1 output dimension and 2 intermediate dimensions. Use ReLU as your intermediate layer activation. \n",
    "\n",
    "Picturing the problem as a classification over two classes, select an appropriate output activation and loss function (refer to the cheat-sheet for aid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=2, out_features=4),\n",
    "    torch.nn.ReLU(),\n",
    "    torch.nn.Linear(in_features=4, out_features=1)\n",
    ").to(device)\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate an optimizer for your network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.SGD(f.parameters(), lr=0.01, momentum=0.9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform 5000 iterations of training, printing the loss as you go."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 loss: 1.4889251025954498e-23\n",
      "Iteration 500 loss: 1.4889251025954498e-23\n",
      "Iteration 1000 loss: 1.4889251025954498e-23\n",
      "Iteration 1500 loss: 1.4889251025954498e-23\n",
      "Iteration 2000 loss: 1.4889251025954498e-23\n",
      "Iteration 2500 loss: 1.4889251025954498e-23\n",
      "Iteration 3000 loss: 1.4889251025954498e-23\n",
      "Iteration 3500 loss: 1.4889251025954498e-23\n",
      "Iteration 4000 loss: 1.4889251025954498e-23\n",
      "Iteration 4500 loss: 1.4889251025954498e-23\n",
      "Iteration 5000 loss: 1.4889251025954498e-23\n"
     ]
    }
   ],
   "source": [
    "for t in range(5001):  # iterate\n",
    "    P = f(X)  # predict \n",
    "    loss = loss_fn(P, Y1)  # compute loss using differently shaped Y\n",
    "    loss.backward()  # backpropagate\n",
    "    opt.step()  # optimize\n",
    "    opt.zero_grad()  # reset gradients\n",
    "    if t%500 == 0:\n",
    "        print('Iteration {} loss: {}'.format(t, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might encounter a shape error here. If you do, don't panic! Read it, understand what the problem is and remember `.view()`.\n",
    "\n",
    "Is the loss improving? If not, try toying around with the intermediate layer's width, the optimizer and its learning rate until your network can solve the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "del f, loss_fn, opt, X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='3d'></a>\n",
    "## 4. Putting Everything Together\n",
    "Time to hone our newly acquired torch skills! \n",
    "\n",
    "We will now put everything together and write an actual network on a real task. The code below is mostly complete, but some parts here and there are missing. You will be asked to fill those in, so pay attention!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, construct a two-layer network that implements the function $f: \\mathbb{R}^{300} \\to \\mathbb{R}$, such that:\n",
    "\n",
    "$f(x) = W_2(ReLU(W_1x + \\beta_1) + \\beta_2$\n",
    "\n",
    "where:\n",
    "* $W_1 \\in \\mathbb{R}^{100, 300}$\n",
    "* $W_2 \\in \\mathbb{R}^{1, 100}$ \n",
    "* $ \\beta_1 \\in \\mathbb{R}^{100}$ \n",
    "* $\\beta_2 \\in \\mathbb{R}^1$\n",
    "\n",
    "\n",
    "using `torch.nn.Sequential` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=300, out_features=100),\n",
    "    torch.nn.Sigmoid(),\n",
    "    torch.nn.Linear(in_features=100, out_features=1),\n",
    "    torch.nn.Sigmoid()\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make some use of this network on real data.\n",
    "We are going to open a data dump containing ~5500 baby names. Each name is associated with a label (either 0 for male, or 1 for female), but also a 300-dimensional vector. Representing words as dense vectors is standard practice in NLP; you will learn more about these vectors in your first assignment. For now, we will simply use them in an attempt to teach the network to distinguish between boy and girl babies given their names, while writing some useful code in the process. üë∂ üçº"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('name_data.p', 'rb') as file:\n",
    "    names, vectors, labels = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do our data look like? Run the snippet below a couple of times to get an impression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 358,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Trace' '1']\n",
      " ['Moishe' '0']\n",
      " ['Hillard' '0']\n",
      " ['Margret' '1']\n",
      " ['Pete' '0']\n",
      " ['Sheppard' '0']\n",
      " ['Ki' '1']\n",
      " ['Rodd' '0']\n",
      " ['Juanita' '0']\n",
      " ['Enya' '1']\n",
      " ['Clyde' '0']\n",
      " ['Niven' '0']\n",
      " ['Nels' '0']\n",
      " ['Alysa' '1']\n",
      " ['Adrianna' '1']\n",
      " ['Zena' '1']\n",
      " ['Carl' '0']\n",
      " ['Shadow' '0']\n",
      " ['Angeline' '1']\n",
      " ['Tilly' '1']]\n"
     ]
    }
   ],
   "source": [
    "print(np.random.permutation(list(zip(names, labels)))[0:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vectors` is a list of numpy arrays, and `labels` is a list of integers. We will need to convert them to lists of FloatTensors. Since that is a lot of data, it is unlikely for them to all fit in the GPU, so we will use the RAM as a temporary storage regardless of your currently used device."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or alternatively, vectors = [torch.tensor(vector) for vector in vectors]\n",
    "vectors = list(map(lambda x: torch.tensor(x, dtype=torch.float), vectors))\n",
    "labels = list(map(lambda x: torch.tensor(x, dtype=torch.float), labels))  \n",
    "#vectors = [torch.tensor(vector) for vector in vectors]\n",
    "#labels = [torch.tensor(labels) for label in labels]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training on your entire dataset is bad practice; we should split the data into a training set and a validation set. We could either do it manually, or let `sklearn` do it for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "names_train, names_val, X_train, X_val, Y_train, Y_val = train_test_split(names, vectors, labels, test_size=0.2)\n",
    "assert len(X_train) == len(Y_train) == len(names_train)\n",
    "assert len(X_val) == len(Y_val) == len(names_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have split the data, we may convert them from a list of tensors onto a big tensor. We could do that by using `view()` to expand the first dimension of each vector and then `cat()` to merge them, but an easier shorthand is `stack()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 361,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4310, 300])\n",
      "torch.Size([1078, 300])\n",
      "torch.Size([4310])\n",
      "torch.Size([1078])\n"
     ]
    }
   ],
   "source": [
    "X_train = torch.stack(X_train)\n",
    "X_val = torch.stack(X_val)\n",
    "Y_train = torch.tensor(Y_train)\n",
    "Y_val = torch.tensor(Y_val)\n",
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(Y_train.shape)\n",
    "print(Y_val.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our tensors in a sensible format, we may construct a Dataset (a storage unit for our data) and a DataLoader (a wrapper responsible for shuffling the data, iterating through them and converting them to batches)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 362,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = TensorDataset(X_train, Y_train)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_dataset = TensorDataset(X_val, Y_val)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=32, shuffle=False)  # no need to shuffle the validation data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin and immediately stop an iteration through the training dataloader to get an idea of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 363,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 300])\n",
      "torch.float32\n",
      "torch.Size([32])\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "for batch_x, batch_y in train_dataloader:\n",
    "    print(batch_x.shape)\n",
    "    print(batch_x.dtype)\n",
    "    print(batch_y.shape)\n",
    "    print(batch_y.dtype)\n",
    "    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking good; batch_x is 32 300-dimensional vectors, and batch_y is 32 values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_batch()` is a function that takes a network, a batch of inputs, a batch of outputs, a loss function and an optimizer, runs the training routine on that batch and returns the loss value of that batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Callable, List\n",
    "\n",
    "# Callable is typed as Callable[[i1, i2, ..], o]\n",
    "# where i1, i2, .. the input types and o the output type\n",
    "\n",
    "def train_batch(network: torch.nn.Module,  # the network\n",
    "                X_batch: torch.FloatTensor,  # the X batch\n",
    "                Y_batch: torch.LongTensor,   # the Y batch\n",
    "                # a function from a FloatTensor (prediction) and a FloatTensor (Y) to a FloatTensor (the loss)\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor],  \n",
    "                # the optimizer\n",
    "                optimizer: torch.optim.Optimizer) -> float:\n",
    "    \n",
    "    network.train()\n",
    "    \n",
    "    prediction_batch = network(X_batch)  # forward pass\n",
    "    batch_loss = loss_fn(prediction_batch.view(-1), Y_batch)  # loss calculation\n",
    "    batch_loss.backward()  # gradient computation\n",
    "    optimizer.step()  # back-propagation\n",
    "    optimizer.zero_grad()  # gradient reset\n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Several network components (e.g. dropout units) may behave differently during training and validation. We use `.train()` to inform the network that we are in training time.\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Warning:</b>\n",
    "The `batch_loss` tensor requires gradient (why?). It is important to return its contents with `.item()` rather than the tensor itself, otherwise we risk memory leak because of the accumulated gradient tracking.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`train_epoch()` is a function that takes a network, the training dataloader, a loss function and an optimizer. It iterates through the dataloader and is responsible for calling `train_batch()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(network: torch.nn.Module, \n",
    "                # a list of data points x\n",
    "                dataloader: DataLoader,\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.FloatTensor], torch.FloatTensor],\n",
    "                optimizer: torch.optim.Optimizer, \n",
    "                device: str) -> float:\n",
    "    \n",
    "    loss = 0.\n",
    "    \n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)  # convert back to your chosen device\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += train_batch(network=network, X_batch=x_batch, Y_batch=y_batch, loss_fn=loss_fn, optimizer=optimizer)\n",
    "    loss /= (i+1) # divide loss by number of batches for consistency \n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your turn; fill in the missing parts of `eval_batch()`, a function that takes a network, a batch of inputs, a batch of outputs and a loss function, and computes the loss of that batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_batch(network: torch.nn.Module,  # the network\n",
    "                X_batch: torch.FloatTensor,  # the X batch\n",
    "                Y_batch: torch.LongTensor,   # the Y batch\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.LongTensor], torch.FloatTensor]) -> float:\n",
    "    \n",
    "    network.eval()\n",
    "    \n",
    "    #because we are in eval mode no backprop is needed\n",
    "    with torch.no_grad(): \n",
    "        prediction_batch = network(X_batch)  # forward pass\n",
    "        batch_loss = loss_fn(prediction_batch.view(-1), Y_batch)  # loss calculation\n",
    "    \n",
    "    return batch_loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Note:</b> Notice that we use `.eval()` to inform the network we are in validation time. Notice also the `no_grad()` context; this is telling torch that it doesn't need to bother with gradient tracking momentarily, providing a significant speed-up for the current session.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`eval_epoch()` is basically the same as `train_epoch()`, aside the lack of an optimizer. Fill it in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_epoch(network: torch.nn.Module, \n",
    "                # a list of data points x\n",
    "                dataloader: DataLoader,\n",
    "                loss_fn: Callable[[torch.FloatTensor, torch.LongTensor], torch.FloatTensor],\n",
    "                device: str) -> float:\n",
    "        \n",
    "    loss = 0.\n",
    "    \n",
    "    # Copied from above with modification due to being eval mode and not training\n",
    "    for i, (x_batch, y_batch) in enumerate(dataloader):\n",
    "        x_batch = x_batch.to(device)  # convert back to your chosen device\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss += eval_batch(network=network, X_batch=x_batch, Y_batch=y_batch, loss_fn=loss_fn)\n",
    "    loss /= (i+1) # divide loss by number of batches for consistency \n",
    "        \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also make an auxilliary `infer_batch()` function; the forward pass gives us the final layer's output, but we might just be interested in the predicted class rather than its probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 368,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer_batch(network: torch.nn.Module, \n",
    "                batch_x: torch.FloatTensor, \n",
    "                device: str) -> torch.LongTensor:\n",
    "    \n",
    "    # first apply the sigmoid activation (since it is implemented by the loss function rather than the network itself)\n",
    "    sigm = torch.sigmoid(network(batch_x.to(device)))\n",
    "    # round the result\n",
    "    classes = torch.round(sigm)\n",
    "    # detach it from the computation graph (we no longer care about its gradients)\n",
    "    classes = classes.detach()\n",
    "    # cast the result into a LongTensor and return\n",
    "    return classes.to(torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One last thing before we can finally train; we need a loss function and an optimizer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = torch.optim.Adam(f.parameters(), lr=1e-05)\n",
    "loss_fn = torch.nn.BCEWithLogitsLoss(reduction='mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      " Training Loss: 0.6981379848939401\n",
      " Validation Loss: 0.7008725597577936\n",
      "Epoch 1\n",
      " Training Loss: 0.6966090339201468\n",
      " Validation Loss: 0.6991684699759764\n",
      "Epoch 2\n",
      " Training Loss: 0.6949677555649368\n",
      " Validation Loss: 0.6976016935180215\n",
      "Epoch 3\n",
      " Training Loss: 0.6936700176309656\n",
      " Validation Loss: 0.6961349704686333\n",
      "Epoch 4\n",
      " Training Loss: 0.6921357556625649\n",
      " Validation Loss: 0.6947596669197083\n",
      "Epoch 5\n",
      " Training Loss: 0.6910350724502846\n",
      " Validation Loss: 0.6935188594986411\n",
      "Epoch 6\n",
      " Training Loss: 0.6898587933293095\n",
      " Validation Loss: 0.6923579994369956\n",
      "Epoch 7\n",
      " Training Loss: 0.68885231812795\n",
      " Validation Loss: 0.6912327794467702\n",
      "Epoch 8\n",
      " Training Loss: 0.687646445080086\n",
      " Validation Loss: 0.6901916370672339\n",
      "Epoch 9\n",
      " Training Loss: 0.686839943461948\n",
      " Validation Loss: 0.6891962289810181\n",
      "Epoch 10\n",
      " Training Loss: 0.685889482498169\n",
      " Validation Loss: 0.6882408191176022\n",
      "Epoch 11\n",
      " Training Loss: 0.6849002458431103\n",
      " Validation Loss: 0.6873443301986245\n",
      "Epoch 12\n",
      " Training Loss: 0.6840074128574796\n",
      " Validation Loss: 0.6864346300854403\n",
      "Epoch 13\n",
      " Training Loss: 0.6832489340393632\n",
      " Validation Loss: 0.6855850991080789\n",
      "Epoch 14\n",
      " Training Loss: 0.6824854281213548\n",
      " Validation Loss: 0.6847608229693245\n",
      "Epoch 15\n",
      " Training Loss: 0.6814950082037184\n",
      " Validation Loss: 0.6839214440654305\n",
      "Epoch 16\n",
      " Training Loss: 0.6808660842754223\n",
      " Validation Loss: 0.6831309918095084\n",
      "Epoch 17\n",
      " Training Loss: 0.6800541175736321\n",
      " Validation Loss: 0.6823160753530615\n",
      "Epoch 18\n",
      " Training Loss: 0.679107298232891\n",
      " Validation Loss: 0.6815110497614917\n",
      "Epoch 19\n",
      " Training Loss: 0.6783488821100305\n",
      " Validation Loss: 0.6806994992143968\n",
      "Epoch 20\n",
      " Training Loss: 0.6775483904061494\n",
      " Validation Loss: 0.679886190330281\n",
      "Epoch 21\n",
      " Training Loss: 0.676666275660197\n",
      " Validation Loss: 0.6790636076646692\n",
      "Epoch 22\n",
      " Training Loss: 0.6758727903719302\n",
      " Validation Loss: 0.6782273369676927\n",
      "Epoch 23\n",
      " Training Loss: 0.6748199356926812\n",
      " Validation Loss: 0.6773822132278892\n",
      "Epoch 24\n",
      " Training Loss: 0.6741878659636886\n",
      " Validation Loss: 0.676521360874176\n",
      "Epoch 25\n",
      " Training Loss: 0.6733108931117587\n",
      " Validation Loss: 0.6756446659564972\n",
      "Epoch 26\n",
      " Training Loss: 0.6722105715009902\n",
      " Validation Loss: 0.6747559151228737\n",
      "Epoch 27\n",
      " Training Loss: 0.6713783105214437\n",
      " Validation Loss: 0.6738530292230493\n",
      "Epoch 28\n",
      " Training Loss: 0.6703725889877037\n",
      " Validation Loss: 0.672925698406556\n",
      "Epoch 29\n",
      " Training Loss: 0.6694314881607338\n",
      " Validation Loss: 0.6719920670284945\n",
      "Epoch 30\n",
      " Training Loss: 0.6683595524893866\n",
      " Validation Loss: 0.6710400406052085\n",
      "Epoch 31\n",
      " Training Loss: 0.6673570743313542\n",
      " Validation Loss: 0.6700730095891392\n",
      "Epoch 32\n",
      " Training Loss: 0.6664622055159675\n",
      " Validation Loss: 0.6690824207137612\n",
      "Epoch 33\n",
      " Training Loss: 0.6655001159067507\n",
      " Validation Loss: 0.6680944597019869\n",
      "Epoch 34\n",
      " Training Loss: 0.6643619272443984\n",
      " Validation Loss: 0.667081768021864\n",
      "Epoch 35\n",
      " Training Loss: 0.6633603859830786\n",
      " Validation Loss: 0.6660628914833069\n",
      "Epoch 36\n",
      " Training Loss: 0.6621802895157426\n",
      " Validation Loss: 0.6650267856962541\n",
      "Epoch 37\n",
      " Training Loss: 0.6611048751407199\n",
      " Validation Loss: 0.6639837114249959\n",
      "Epoch 38\n",
      " Training Loss: 0.6600156505902608\n",
      " Validation Loss: 0.6629348660216612\n",
      "Epoch 39\n",
      " Training Loss: 0.6589671850204468\n",
      " Validation Loss: 0.6618738770484924\n",
      "Epoch 40\n",
      " Training Loss: 0.6577245336991769\n",
      " Validation Loss: 0.6608153493965373\n",
      "Epoch 41\n",
      " Training Loss: 0.6566800192550376\n",
      " Validation Loss: 0.6597484104773578\n",
      "Epoch 42\n",
      " Training Loss: 0.65549364178269\n",
      " Validation Loss: 0.6586829231065863\n",
      "Epoch 43\n",
      " Training Loss: 0.6542844216028849\n",
      " Validation Loss: 0.6576093698249144\n",
      "Epoch 44\n",
      " Training Loss: 0.6531094091909903\n",
      " Validation Loss: 0.6565223213504342\n",
      "Epoch 45\n",
      " Training Loss: 0.6519515439316078\n",
      " Validation Loss: 0.6554545392008388\n",
      "Epoch 46\n",
      " Training Loss: 0.650996904902988\n",
      " Validation Loss: 0.6543807439944324\n",
      "Epoch 47\n",
      " Training Loss: 0.6497810147426747\n",
      " Validation Loss: 0.6533001941793105\n",
      "Epoch 48\n",
      " Training Loss: 0.648762352819796\n",
      " Validation Loss: 0.6522216323543998\n",
      "Epoch 49\n",
      " Training Loss: 0.6475528937798959\n",
      " Validation Loss: 0.651150552665486\n",
      "Epoch 50\n",
      " Training Loss: 0.6464758087087561\n",
      " Validation Loss: 0.6500784930060891\n",
      "Epoch 51\n",
      " Training Loss: 0.645239715664475\n",
      " Validation Loss: 0.6490058443125557\n",
      "Epoch 52\n",
      " Training Loss: 0.6441400784033317\n",
      " Validation Loss: 0.6479300926713383\n",
      "Epoch 53\n",
      " Training Loss: 0.6429493996832106\n",
      " Validation Loss: 0.646860841442557\n",
      "Epoch 54\n",
      " Training Loss: 0.6420349483136778\n",
      " Validation Loss: 0.6457979223307442\n",
      "Epoch 55\n",
      " Training Loss: 0.6406452382052387\n",
      " Validation Loss: 0.644738528658362\n",
      "Epoch 56\n",
      " Training Loss: 0.6396292231701038\n",
      " Validation Loss: 0.6436824623276206\n",
      "Epoch 57\n",
      " Training Loss: 0.6385587784979079\n",
      " Validation Loss: 0.64263935825404\n",
      "Epoch 58\n",
      " Training Loss: 0.6376737400337502\n",
      " Validation Loss: 0.6416009752189412\n",
      "Epoch 59\n",
      " Training Loss: 0.6365072104665969\n",
      " Validation Loss: 0.6405698674566606\n",
      "Epoch 60\n",
      " Training Loss: 0.6353620056752806\n",
      " Validation Loss: 0.6395442994201884\n",
      "Epoch 61\n",
      " Training Loss: 0.6340769542588128\n",
      " Validation Loss: 0.6385274652172538\n",
      "Epoch 62\n",
      " Training Loss: 0.6331790442819949\n",
      " Validation Loss: 0.637517231352189\n",
      "Epoch 63\n",
      " Training Loss: 0.6321991231706408\n",
      " Validation Loss: 0.6365363194661982\n",
      "Epoch 64\n",
      " Training Loss: 0.6312843049014056\n",
      " Validation Loss: 0.6355441808700562\n",
      "Epoch 65\n",
      " Training Loss: 0.6300337420569526\n",
      " Validation Loss: 0.6345755773432115\n",
      "Epoch 66\n",
      " Training Loss: 0.629194830082081\n",
      " Validation Loss: 0.6336042810888851\n",
      "Epoch 67\n",
      " Training Loss: 0.6279917672828391\n",
      " Validation Loss: 0.6326423988622778\n",
      "Epoch 68\n",
      " Training Loss: 0.627034721992634\n",
      " Validation Loss: 0.6316877778838662\n",
      "Epoch 69\n",
      " Training Loss: 0.6258685297436184\n",
      " Validation Loss: 0.6307461437057046\n",
      "Epoch 70\n",
      " Training Loss: 0.6249123732248942\n",
      " Validation Loss: 0.6298095618977266\n",
      "Epoch 71\n",
      " Training Loss: 0.623992074418951\n",
      " Validation Loss: 0.6288853094858282\n",
      "Epoch 72\n",
      " Training Loss: 0.6230400231149461\n",
      " Validation Loss: 0.6279774290673873\n",
      "Epoch 73\n",
      " Training Loss: 0.6222061541345384\n",
      " Validation Loss: 0.6270730372737435\n",
      "Epoch 74\n",
      " Training Loss: 0.6211557851897346\n",
      " Validation Loss: 0.6261719149701735\n",
      "Epoch 75\n",
      " Training Loss: 0.620088141935843\n",
      " Validation Loss: 0.6252996378085193\n",
      "Epoch 76\n",
      " Training Loss: 0.6193242364459568\n",
      " Validation Loss: 0.6244190826135523\n",
      "Epoch 77\n",
      " Training Loss: 0.6183458858066135\n",
      " Validation Loss: 0.6235543454394621\n",
      "Epoch 78\n",
      " Training Loss: 0.6174457196836118\n",
      " Validation Loss: 0.6226934457526487\n",
      "Epoch 79\n",
      " Training Loss: 0.6165918796150772\n",
      " Validation Loss: 0.6218415218241075\n",
      "Epoch 80\n",
      " Training Loss: 0.6155293416093897\n",
      " Validation Loss: 0.6210077247198891\n",
      "Epoch 81\n",
      " Training Loss: 0.6146786062805741\n",
      " Validation Loss: 0.6201751179554883\n",
      "Epoch 82\n",
      " Training Loss: 0.6136863258149888\n",
      " Validation Loss: 0.6193600507343516\n",
      "Epoch 83\n",
      " Training Loss: 0.6129484066256771\n",
      " Validation Loss: 0.6185558648670421\n",
      "Epoch 84\n",
      " Training Loss: 0.6121078195395293\n",
      " Validation Loss: 0.6177641749382019\n",
      "Epoch 85\n",
      " Training Loss: 0.6112498941244903\n",
      " Validation Loss: 0.6169812486452215\n",
      "Epoch 86\n",
      " Training Loss: 0.6107189990856029\n",
      " Validation Loss: 0.6162103116512299\n",
      "Epoch 87\n",
      " Training Loss: 0.6096165400964243\n",
      " Validation Loss: 0.61545589390923\n",
      "Epoch 88\n",
      " Training Loss: 0.6087601374696803\n",
      " Validation Loss: 0.6147141877342673\n",
      "Epoch 89\n",
      " Training Loss: 0.6082413646909925\n",
      " Validation Loss: 0.6139795587343329\n",
      "Epoch 90\n",
      " Training Loss: 0.6073011354163841\n",
      " Validation Loss: 0.6132703563746285\n",
      "Epoch 91\n",
      " Training Loss: 0.6063501115198489\n",
      " Validation Loss: 0.6125644971342648\n",
      "Epoch 92\n",
      " Training Loss: 0.6057898212362218\n",
      " Validation Loss: 0.61187123726396\n",
      "Epoch 93\n",
      " Training Loss: 0.6051223472312645\n",
      " Validation Loss: 0.6111867515479817\n",
      "Epoch 94\n",
      " Training Loss: 0.6043410901670103\n",
      " Validation Loss: 0.6105131769881529\n",
      "Epoch 95\n",
      " Training Loss: 0.6035966904075057\n",
      " Validation Loss: 0.6098513550618115\n",
      "Epoch 96\n",
      " Training Loss: 0.6028946271649114\n",
      " Validation Loss: 0.6091962071026072\n",
      "Epoch 97\n",
      " Training Loss: 0.6021520318808379\n",
      " Validation Loss: 0.6085554343812606\n",
      "Epoch 98\n",
      " Training Loss: 0.6014553149541219\n",
      " Validation Loss: 0.6079236882574418\n",
      "Epoch 99\n",
      " Training Loss: 0.6007256618252507\n",
      " Validation Loss: 0.607298624866149\n"
     ]
    }
   ],
   "source": [
    "NUM_EPOCHS = 100\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for t in range(NUM_EPOCHS):\n",
    "    train_loss = train_epoch(f, train_dataloader, optimizer=opt, loss_fn=loss_fn, device=device)\n",
    "    val_loss = eval_epoch(f, val_dataloader, loss_fn, device=device)\n",
    "    \n",
    "    print('Epoch {}'.format(t))\n",
    "    print(' Training Loss: {}'.format(train_loss))\n",
    "    print(' Validation Loss: {}'.format(val_loss))\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may plot the losses to get an idea of what the learning curve looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.legend(['Training', 'Validation'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And let's bring this to an end by labeling all of our validation data and printing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Dewey', [1]),\n",
      " ('Juliane', [1]),\n",
      " ('Ginnifer', [1]),\n",
      " ('Jonny', [1]),\n",
      " ('Nate', [1]),\n",
      " ('Alleen', [1]),\n",
      " ('Devan', [1]),\n",
      " ('Zoe', [1]),\n",
      " ('Abigail', [1]),\n",
      " ('France', [1]),\n",
      " ('Angelo', [1]),\n",
      " ('Crissy', [1]),\n",
      " ('Magdalen', [1]),\n",
      " ('Shawn', [1]),\n",
      " ('Julieta', [1]),\n",
      " ('Leonhard', [1]),\n",
      " ('Berri', [1]),\n",
      " ('Cy', [1]),\n",
      " ('Casey', [1]),\n",
      " ('Xerxes', [1]),\n",
      " ('Nels', [1]),\n",
      " ('Dallas', [1]),\n",
      " ('Julia', [1]),\n",
      " ('Geeta', [1]),\n",
      " ('Ignace', [1]),\n",
      " ('Lay', [1]),\n",
      " ('Cele', [1]),\n",
      " ('Arlie', [1]),\n",
      " ('Suzi', [1]),\n",
      " ('Ariel', [1]),\n",
      " ('Esther', [1]),\n",
      " ('Shayla', [1]),\n",
      " ('Xavier', [1]),\n",
      " ('Star', [1]),\n",
      " ('Cammy', [1]),\n",
      " ('Jewell', [1]),\n",
      " ('Huntley', [1]),\n",
      " ('Carey', [1]),\n",
      " ('Davy', [1]),\n",
      " ('Mabel', [1]),\n",
      " ('Clement', [1]),\n",
      " ('Otes', [1]),\n",
      " ('Carmela', [1]),\n",
      " ('Courtenay', [1]),\n",
      " ('Torey', [1]),\n",
      " ('Serene', [1]),\n",
      " ('Norris', [1]),\n",
      " ('Dominick', [1]),\n",
      " ('Cayla', [1]),\n",
      " ('Carter', [1]),\n",
      " ('Sarina', [1]),\n",
      " ('Domenico', [1]),\n",
      " ('Queenie', [1]),\n",
      " ('Em', [1]),\n",
      " ('Aliza', [1]),\n",
      " ('Olive', [1]),\n",
      " ('Carie', [1]),\n",
      " ('Desmond', [1]),\n",
      " ('Cain', [1]),\n",
      " ('Leone', [1]),\n",
      " ('Zach', [1]),\n",
      " ('Dory', [1]),\n",
      " ('Emilio', [1]),\n",
      " ('Socrates', [1]),\n",
      " ('Marisa', [1]),\n",
      " ('Herrick', [1]),\n",
      " ('Florian', [1]),\n",
      " ('Bertha', [1]),\n",
      " ('Zeb', [1]),\n",
      " ('Cassey', [1]),\n",
      " ('Stefanie', [1]),\n",
      " ('Tyne', [1]),\n",
      " ('Kaiser', [1]),\n",
      " ('Leopold', [1]),\n",
      " ('Hugo', [1]),\n",
      " ('Mavis', [1]),\n",
      " ('Pier', [1]),\n",
      " ('Torin', [1]),\n",
      " ('Peggy', [1]),\n",
      " ('Terri', [1]),\n",
      " ('Emilie', [1]),\n",
      " ('Meyer', [1]),\n",
      " ('Manon', [1]),\n",
      " ('Pavia', [1]),\n",
      " ('Row', [1]),\n",
      " ('Filipe', [1]),\n",
      " ('Collins', [1]),\n",
      " ('Elyse', [1]),\n",
      " ('Chantal', [1]),\n",
      " ('Gregorio', [1]),\n",
      " ('Bharat', [1]),\n",
      " ('Marta', [1]),\n",
      " ('Rebeca', [1]),\n",
      " ('Lothar', [1]),\n",
      " ('Romona', [1]),\n",
      " ('Duncan', [1]),\n",
      " ('Briney', [1]),\n",
      " ('Charlene', [1]),\n",
      " ('Ernest', [1]),\n",
      " ('Murdoch', [1]),\n",
      " ('Helge', [1]),\n",
      " ('Emmi', [1]),\n",
      " ('Dacia', [1]),\n",
      " ('Waylon', [1]),\n",
      " ('Alena', [1]),\n",
      " ('Shepherd', [1]),\n",
      " ('Francesco', [1]),\n",
      " ('Lowell', [1]),\n",
      " ('Lorne', [1]),\n",
      " ('Simone', [1]),\n",
      " ('Erina', [1]),\n",
      " ('Elly', [1]),\n",
      " ('Morten', [1]),\n",
      " ('Abdul', [1]),\n",
      " ('Luna', [1]),\n",
      " ('Jackie', [1]),\n",
      " ('Roosevelt', [1]),\n",
      " ('Roth', [1]),\n",
      " ('Shellie', [1]),\n",
      " ('Jean-Paul', [1]),\n",
      " ('Rickey', [1]),\n",
      " ('Corey', [1]),\n",
      " ('Armand', [1]),\n",
      " ('Frankie', [1]),\n",
      " ('Rollo', [1]),\n",
      " ('Willa', [1]),\n",
      " ('Nickey', [1]),\n",
      " ('Bartlett', [1]),\n",
      " ('Emil', [1]),\n",
      " ('Addie', [1]),\n",
      " ('Goddard', [1]),\n",
      " ('Russell', [1]),\n",
      " ('Ariadne', [1]),\n",
      " ('Seymour', [1]),\n",
      " ('Carroll', [1]),\n",
      " ('Pacifica', [1]),\n",
      " ('Bettina', [1]),\n",
      " ('Leyla', [1]),\n",
      " ('Robbie', [1]),\n",
      " ('Andrus', [1]),\n",
      " ('Augustine', [1]),\n",
      " ('Abe', [1]),\n",
      " ('Laurel', [1]),\n",
      " ('Forbes', [1]),\n",
      " ('Joan', [1]),\n",
      " ('Nadya', [1]),\n",
      " ('Letty', [1]),\n",
      " ('Meggan', [1]),\n",
      " ('Birgit', [1]),\n",
      " ('Stormie', [1]),\n",
      " ('Flo', [1]),\n",
      " ('Waldo', [1]),\n",
      " ('Mia', [1]),\n",
      " ('Mayer', [1]),\n",
      " ('Zebulon', [1]),\n",
      " ('Stearn', [1]),\n",
      " ('Madalena', [1]),\n",
      " ('Randell', [1]),\n",
      " ('Riley', [1]),\n",
      " ('Sparky', [1]),\n",
      " ('Edith', [1]),\n",
      " ('Gustavo', [1]),\n",
      " ('Leola', [1]),\n",
      " ('Benjamin', [1]),\n",
      " ('Mairead', [1]),\n",
      " ('Caroline', [1]),\n",
      " ('Jeannie', [1]),\n",
      " ('Maud', [1]),\n",
      " ('Aubert', [1]),\n",
      " ('Rollins', [1]),\n",
      " ('Fred', [1]),\n",
      " ('Coretta', [1]),\n",
      " ('Harvie', [1]),\n",
      " ('Melonie', [1]),\n",
      " ('Micah', [1]),\n",
      " ('Nealon', [1]),\n",
      " ('Juanita', [1]),\n",
      " ('Desdemona', [1]),\n",
      " ('Uriah', [1]),\n",
      " ('Hilbert', [1]),\n",
      " ('Deena', [1]),\n",
      " ('Remy', [1]),\n",
      " ('Shawna', [1]),\n",
      " ('Niels', [1]),\n",
      " ('Roland', [1]),\n",
      " ('Reggie', [1]),\n",
      " ('Dimitri', [1]),\n",
      " ('Miran', [1]),\n",
      " ('Ina', [1]),\n",
      " ('Timi', [1]),\n",
      " ('Levy', [1]),\n",
      " ('Conroy', [1]),\n",
      " ('Rayna', [1]),\n",
      " ('Tricia', [1]),\n",
      " ('Kathryn', [1]),\n",
      " ('Michell', [1]),\n",
      " ('Sarah', [1]),\n",
      " ('Tracie', [1]),\n",
      " ('Lewis', [1]),\n",
      " ('Marcus', [1]),\n",
      " ('Kimmy', [1]),\n",
      " ('Oleg', [1]),\n",
      " ('Gerry', [1]),\n",
      " ('Dominique', [1]),\n",
      " ('Yves', [1]),\n",
      " ('Joshua', [1]),\n",
      " ('Thorpe', [1]),\n",
      " ('Missie', [1]),\n",
      " ('Mateo', [1]),\n",
      " ('Nanci', [1]),\n",
      " ('Querida', [1]),\n",
      " ('Mikey', [1]),\n",
      " ('Timmy', [1]),\n",
      " ('Andres', [1]),\n",
      " ('Lamont', [1]),\n",
      " ('Damien', [1]),\n",
      " ('Rock', [1]),\n",
      " ('Filippa', [1]),\n",
      " ('Flower', [1]),\n",
      " ('Kincaid', [1]),\n",
      " ('Gustav', [1]),\n",
      " ('Hamil', [1]),\n",
      " ('Jenette', [1]),\n",
      " ('Brian', [1]),\n",
      " ('Eryn', [1]),\n",
      " ('Mickey', [1]),\n",
      " ('Celia', [1]),\n",
      " ('Bogart', [1]),\n",
      " ('Vilma', [1]),\n",
      " ('Dore', [1]),\n",
      " ('Ricki', [1]),\n",
      " ('Gert', [1]),\n",
      " ('Winfred', [1]),\n",
      " ('Ajai', [1]),\n",
      " ('Susy', [1]),\n",
      " ('Vaughn', [1]),\n",
      " ('Sutherland', [1]),\n",
      " ('Liz', [1]),\n",
      " ('Clarke', [1]),\n",
      " ('Leslie', [1]),\n",
      " ('Ira', [1]),\n",
      " ('Malka', [1]),\n",
      " ('Dolley', [1]),\n",
      " ('Matt', [1]),\n",
      " ('Jerald', [1]),\n",
      " ('Helmuth', [1]),\n",
      " ('Barbe', [1]),\n",
      " ('Vaclav', [1]),\n",
      " ('Salome', [1]),\n",
      " ('Minerva', [1]),\n",
      " ('Persis', [1]),\n",
      " ('Serge', [1]),\n",
      " ('Hamid', [1]),\n",
      " ('Marco', [1]),\n",
      " ('Perl', [1]),\n",
      " ('Janie', [1]),\n",
      " ('Lenora', [1]),\n",
      " ('Gillan', [1]),\n",
      " ('Luana', [1]),\n",
      " ('Christi', [1]),\n",
      " ('Maryjane', [1]),\n",
      " ('Martino', [1]),\n",
      " ('Angela', [1]),\n",
      " ('Albert', [1]),\n",
      " ('Tracie', [1]),\n",
      " ('Dorothy', [1]),\n",
      " ('Merril', [1]),\n",
      " ('Dario', [1]),\n",
      " ('Madonna', [1]),\n",
      " ('Yancey', [1]),\n",
      " ('Farley', [1]),\n",
      " ('Eugen', [1]),\n",
      " ('Mitchell', [1]),\n",
      " ('Sophia', [1]),\n",
      " ('Cathrine', [1]),\n",
      " ('Sapphire', [1]),\n",
      " ('Sammy', [1]),\n",
      " ('Christof', [1]),\n",
      " ('Penny', [1]),\n",
      " ('Bennett', [1]),\n",
      " ('Derby', [1]),\n",
      " ('Anne-Marie', [1]),\n",
      " ('Ajay', [1]),\n",
      " ('Essie', [1]),\n",
      " ('Leonora', [1]),\n",
      " ('Alexis', [1]),\n",
      " ('Terra', [1]),\n",
      " ('Manuel', [1]),\n",
      " ('Zahara', [1]),\n",
      " ('Travis', [1]),\n",
      " ('Ford', [1]),\n",
      " ('Jan', [1]),\n",
      " ('Abbie', [1]),\n",
      " ('Godwin', [1]),\n",
      " ('Fergus', [1]),\n",
      " ('Dudley', [1]),\n",
      " ('Slade', [1]),\n",
      " ('Lotte', [1]),\n",
      " ('Jaimie', [1]),\n",
      " ('Vanny', [1]),\n",
      " ('Susann', [1]),\n",
      " ('Siouxie', [1]),\n",
      " ('Baron', [1]),\n",
      " ('Pail', [1]),\n",
      " ('Cory', [1]),\n",
      " ('Angie', [1]),\n",
      " ('Nev', [1]),\n",
      " ('Ilana', [1]),\n",
      " ('Irvin', [1]),\n",
      " ('Genny', [1]),\n",
      " ('Del', [1]),\n",
      " ('Schuyler', [1]),\n",
      " ('Stanly', [1]),\n",
      " ('Matty', [1]),\n",
      " ('Desirae', [1]),\n",
      " ('Cameron', [1]),\n",
      " ('Karna', [1]),\n",
      " ('Randi', [1]),\n",
      " ('Cami', [1]),\n",
      " ('Marja', [1]),\n",
      " ('Joanne', [1]),\n",
      " ('Chet', [1]),\n",
      " ('Maddalena', [1]),\n",
      " ('Bethany', [1]),\n",
      " ('Sue', [1]),\n",
      " ('David', [1]),\n",
      " ('Redford', [1]),\n",
      " ('Giraldo', [1]),\n",
      " ('Sophie', [1]),\n",
      " ('Tan', [1]),\n",
      " ('Billie', [1]),\n",
      " ('Christy', [1]),\n",
      " ('Dee', [1]),\n",
      " ('Minda', [1]),\n",
      " ('Roz', [1]),\n",
      " ('Sara-Ann', [1]),\n",
      " ('Lin', [1]),\n",
      " ('Paule', [1]),\n",
      " ('Jae', [1]),\n",
      " ('Mattie', [1]),\n",
      " ('Roberta', [1]),\n",
      " ('Kathy', [1]),\n",
      " ('Aditya', [1]),\n",
      " ('Franklin', [1]),\n",
      " ('Armando', [1]),\n",
      " ('Vinnie', [1]),\n",
      " ('Nathanael', [1]),\n",
      " ('Hewitt', [1]),\n",
      " ('Clyde', [1]),\n",
      " ('Donny', [1]),\n",
      " ('Cris', [1]),\n",
      " ('Kristan', [1]),\n",
      " ('Manda', [1]),\n",
      " ('Harmonie', [1]),\n",
      " ('Marion', [1]),\n",
      " ('Shannen', [1]),\n",
      " ('Ajay', [1]),\n",
      " ('Sly', [1]),\n",
      " ('Daniele', [1]),\n",
      " ('Sandor', [1]),\n",
      " ('Gerda', [1]),\n",
      " ('La', [1]),\n",
      " ('Merle', [1]),\n",
      " ('Zola', [1]),\n",
      " ('Verla', [1]),\n",
      " ('Barbi', [1]),\n",
      " ('Smitty', [1]),\n",
      " ('Emmett', [1]),\n",
      " ('Christian', [1]),\n",
      " ('Annamarie', [1]),\n",
      " ('Lauren', [1]),\n",
      " ('Bela', [1]),\n",
      " ('Rozelle', [1]),\n",
      " ('Pace', [1]),\n",
      " ('Celina', [1]),\n",
      " ('Asia', [1]),\n",
      " ('Reuben', [1]),\n",
      " ('Travers', [1]),\n",
      " ('Netti', [1]),\n",
      " ('Park', [1]),\n",
      " ('Garret', [1]),\n",
      " ('Elmer', [1]),\n",
      " ('Nesta', [1]),\n",
      " ('Giordano', [1]),\n",
      " ('Elissa', [1]),\n",
      " ('Etty', [1]),\n",
      " ('Lori', [1]),\n",
      " ('Jordan', [1]),\n",
      " ('Tally', [1]),\n",
      " ('Hayes', [1]),\n",
      " ('Sharia', [1]),\n",
      " ('Timmie', [1]),\n",
      " ('Lil', [1]),\n",
      " ('Acacia', [1]),\n",
      " ('Judie', [1]),\n",
      " ('Shep', [1]),\n",
      " ('Sascha', [1]),\n",
      " ('Meara', [1]),\n",
      " ('Lois', [1]),\n",
      " ('Elysia', [1]),\n",
      " ('Lenard', [1]),\n",
      " ('Page', [1]),\n",
      " ('Maia', [1]),\n",
      " ('Yule', [1]),\n",
      " ('Lira', [1]),\n",
      " ('Emory', [1]),\n",
      " ('Nerissa', [1]),\n",
      " ('Catherine', [1]),\n",
      " ('Chester', [1]),\n",
      " ('Charlton', [1]),\n",
      " ('Tucky', [1]),\n",
      " ('Cleland', [1]),\n",
      " ('Mead', [1]),\n",
      " ('Homer', [1]),\n",
      " ('Hayden', [1]),\n",
      " ('Vivian', [1]),\n",
      " ('Randolf', [1]),\n",
      " ('Silvan', [1]),\n",
      " ('Rudd', [1]),\n",
      " ('Marigold', [1]),\n",
      " ('Mair', [1]),\n",
      " ('Hyacinth', [1]),\n",
      " ('Jay', [1]),\n",
      " ('Harcourt', [1]),\n",
      " ('Rafa', [1]),\n",
      " ('Trenton', [1]),\n",
      " ('Adolphe', [1]),\n",
      " ('Genna', [1]),\n",
      " ('Karrie', [1]),\n",
      " ('Husain', [1]),\n",
      " ('Jesus', [1]),\n",
      " ('Julee', [1]),\n",
      " ('Fara', [1]),\n",
      " ('Romy', [1]),\n",
      " ('Johann', [1]),\n",
      " ('Skipp', [1]),\n",
      " ('Janessa', [1]),\n",
      " ('Pincus', [1]),\n",
      " ('Francesca', [1]),\n",
      " ('Nicolette', [1]),\n",
      " ('Rosy', [1]),\n",
      " ('Lena', [1]),\n",
      " ('Geraldo', [1]),\n",
      " ('Alexi', [1]),\n",
      " ('Veronika', [1]),\n",
      " ('Hali', [1]),\n",
      " ('Katha', [1]),\n",
      " ('Marsh', [1]),\n",
      " ('Nathaniel', [1]),\n",
      " ('Raymund', [1]),\n",
      " ('Roscoe', [1]),\n",
      " ('Kelsey', [1]),\n",
      " ('Nahum', [1]),\n",
      " ('Trip', [1]),\n",
      " ('Joe', [1]),\n",
      " ('Martina', [1]),\n",
      " ('Elana', [1]),\n",
      " ('Isaak', [1]),\n",
      " ('Nada', [1]),\n",
      " ('Regine', [1]),\n",
      " ('Dania', [1]),\n",
      " ('Ethel', [1]),\n",
      " ('Georgia', [1]),\n",
      " ('Cecile', [1]),\n",
      " ('Cookie', [1]),\n",
      " ('Stacia', [1]),\n",
      " ('Naomi', [1]),\n",
      " ('Ahmed', [1]),\n",
      " ('Melisa', [1]),\n",
      " ('Rusty', [1]),\n",
      " ('Lilly', [1]),\n",
      " ('Lawson', [1]),\n",
      " ('Quinn', [1]),\n",
      " ('Jill', [1]),\n",
      " ('Ami', [1]),\n",
      " ('Abbie', [1]),\n",
      " ('Barbara-Anne', [1]),\n",
      " ('Adrian', [1]),\n",
      " ('Lottie', [1]),\n",
      " ('Ronny', [1]),\n",
      " ('Ty', [1]),\n",
      " ('Wilfred', [1]),\n",
      " ('Daniel', [1]),\n",
      " ('Mirabelle', [1]),\n",
      " ('Angelita', [1]),\n",
      " ('Annabelle', [1]),\n",
      " ('Ira', [1]),\n",
      " ('Muriel', [1]),\n",
      " ('Maison', [1]),\n",
      " ('Blake', [1]),\n",
      " ('Sergeant', [1]),\n",
      " ('Anabelle', [1]),\n",
      " ('Shayna', [1]),\n",
      " ('Archon', [1]),\n",
      " ('Northrup', [1]),\n",
      " ('Jerzy', [1]),\n",
      " ('Verena', [1]),\n",
      " ('Roseanna', [1]),\n",
      " ('Billy', [1]),\n",
      " ('Fan', [1]),\n",
      " ('Abram', [1]),\n",
      " ('Lacie', [1]),\n",
      " ('Jammie', [1]),\n",
      " ('Walther', [1]),\n",
      " ('Mahmud', [1]),\n",
      " ('Aristotle', [1]),\n",
      " ('Sissy', [1]),\n",
      " ('Betta', [1]),\n",
      " ('Sterne', [1]),\n",
      " ('Sean', [1]),\n",
      " ('Elfie', [1]),\n",
      " ('Cosmo', [1]),\n",
      " ('Connolly', [1]),\n",
      " ('Claude', [1]),\n",
      " ('Parke', [1]),\n",
      " ('Chastity', [1]),\n",
      " ('Joye', [1]),\n",
      " ('Graig', [1]),\n",
      " ('Grady', [1]),\n",
      " ('Harald', [1]),\n",
      " ('Rudyard', [1]),\n",
      " ('Venkat', [1]),\n",
      " ('Hall', [1]),\n",
      " ('Jessee', [1]),\n",
      " ('Minni', [1]),\n",
      " ('Antoinette', [1]),\n",
      " ('Marius', [1]),\n",
      " ('Brooks', [1]),\n",
      " ('Maribeth', [1]),\n",
      " ('Fremont', [1]),\n",
      " ('Delly', [1]),\n",
      " ('Sada', [1]),\n",
      " ('Pandora', [1]),\n",
      " ('Josef', [1]),\n",
      " ('Fernanda', [1]),\n",
      " ('Sloane', [1]),\n",
      " ('Lanni', [1]),\n",
      " ('Dede', [1]),\n",
      " ('Sanson', [1]),\n",
      " ('Rosette', [1]),\n",
      " ('Philippine', [1]),\n",
      " ('Allie', [1]),\n",
      " ('Winnie', [1]),\n",
      " ('Hakim', [1]),\n",
      " ('Laure', [1]),\n",
      " ('Peg', [1]),\n",
      " ('Tobi', [1]),\n",
      " ('Carine', [1]),\n",
      " ('Waring', [1]),\n",
      " ('Kin', [1]),\n",
      " ('Gilbert', [1]),\n",
      " ('Clancy', [1]),\n",
      " ('Arlee', [1]),\n",
      " ('Filbert', [1]),\n",
      " ('Sky', [1]),\n",
      " ('Sylvie', [1]),\n",
      " ('Fifi', [1]),\n",
      " ('Phillie', [1]),\n",
      " ('Loren', [1]),\n",
      " ('Barri', [1]),\n",
      " ('Ben', [1]),\n",
      " ('Benny', [1]),\n",
      " ('Christin', [1]),\n",
      " ('Rhody', [1]),\n",
      " ('Twila', [1]),\n",
      " ('Richy', [1]),\n",
      " ('French', [1]),\n",
      " ('Olivier', [1]),\n",
      " ('Mack', [1]),\n",
      " ('Carla', [1]),\n",
      " ('Neil', [1]),\n",
      " ('Elsie', [1]),\n",
      " ('Wilma', [1]),\n",
      " ('Melanie', [1]),\n",
      " ('Alis', [1]),\n",
      " ('Mason', [1]),\n",
      " ('Edmund', [1]),\n",
      " ('Hart', [1]),\n",
      " ('Craig', [1]),\n",
      " ('Lorena', [1]),\n",
      " ('Aurora', [1]),\n",
      " ('Rosella', [1]),\n",
      " ('Pippa', [1]),\n",
      " ('Peter', [1]),\n",
      " ('Berna', [1]),\n",
      " ('Lesli', [1]),\n",
      " ('Collette', [1]),\n",
      " ('Lucina', [1]),\n",
      " ('Valentine', [1]),\n",
      " ('Darth', [1]),\n",
      " ('Lane', [1]),\n",
      " ('Kathrine', [1]),\n",
      " ('Louis', [1]),\n",
      " ('Allah', [1]),\n",
      " ('Wendy', [1]),\n",
      " ('Renee', [1]),\n",
      " ('Daffy', [1]),\n",
      " ('John-Patrick', [1]),\n",
      " ('Cobb', [1]),\n",
      " ('Malcolm', [1]),\n",
      " ('JoAnn', [1]),\n",
      " ('Tatiana', [1]),\n",
      " ('Shannon', [1]),\n",
      " ('Tre', [1]),\n",
      " ('Peyton', [1]),\n",
      " ('Holly', [1]),\n",
      " ('Randi', [1]),\n",
      " ('Corbin', [1]),\n",
      " ('Zachery', [1]),\n",
      " ('Jermaine', [1]),\n",
      " ('Dirk', [1]),\n",
      " ('Maria', [1]),\n",
      " ('Kimmie', [1]),\n",
      " ('Clementina', [1]),\n",
      " ('Hogan', [1]),\n",
      " ('Fey', [1]),\n",
      " ('Brinkley', [1]),\n",
      " ('Frieda', [1]),\n",
      " ('Piggy', [1]),\n",
      " ('Aura', [1]),\n",
      " ('Margie', [1]),\n",
      " ('Gillie', [1]),\n",
      " ('Lucien', [1]),\n",
      " ('Hedy', [1]),\n",
      " ('Cheri', [1]),\n",
      " ('Shir', [1]),\n",
      " ('Ronen', [1]),\n",
      " ('Merill', [1]),\n",
      " ('Tani', [1]),\n",
      " ('Dorcas', [1]),\n",
      " ('Bria', [1]),\n",
      " ('Edi', [1]),\n",
      " ('Stafford', [1]),\n",
      " ('Arie', [1]),\n",
      " ('Mayor', [1]),\n",
      " ('Eveline', [1]),\n",
      " ('Billie', [1]),\n",
      " ('Rosita', [1]),\n",
      " ('Northrop', [1]),\n",
      " ('Jean-Marc', [1]),\n",
      " ('Noni', [1]),\n",
      " ('Gera', [1]),\n",
      " ('Way', [1]),\n",
      " ('Aphrodite', [1]),\n",
      " ('Jenna', [1]),\n",
      " ('Julie', [1]),\n",
      " ('Nanni', [1]),\n",
      " ('Annalise', [1]),\n",
      " ('Ikey', [1]),\n",
      " ('Linea', [1]),\n",
      " ('Celestine', [1]),\n",
      " ('Shell', [1]),\n",
      " ('Eric', [1]),\n",
      " ('Swen', [1]),\n",
      " ('Catharine', [1]),\n",
      " ('Viviana', [1]),\n",
      " ('Herb', [1]),\n",
      " ('Gabriel', [1]),\n",
      " ('Eba', [1]),\n",
      " ('Nadia', [1]),\n",
      " ('Marshal', [1]),\n",
      " ('Remus', [1]),\n",
      " ('Connie', [1]),\n",
      " ('Cat', [1]),\n",
      " ('Elliott', [1]),\n",
      " ('Allan', [1]),\n",
      " ('Marlene', [1]),\n",
      " ('Hallie', [1]),\n",
      " ('Mada', [1]),\n",
      " ('Merry', [1]),\n",
      " ('Kala', [1]),\n",
      " ('Marion', [1]),\n",
      " ('Nikolai', [1]),\n",
      " ('Cinnamon', [1]),\n",
      " ('Gabi', [1]),\n",
      " ('Boris', [1]),\n",
      " ('Ola', [1]),\n",
      " ('Alexei', [1]),\n",
      " ('Sinclair', [1]),\n",
      " ('Quintana', [1]),\n",
      " ('Shandy', [1]),\n",
      " ('Erik', [1]),\n",
      " ('Holly-Anne', [1]),\n",
      " ('Ned', [1]),\n",
      " ('Ada', [1]),\n",
      " ('Audi', [1]),\n",
      " ('Wilhelmina', [1]),\n",
      " ('Una', [1]),\n",
      " ('Xenos', [1]),\n",
      " ('Rikki', [1]),\n",
      " ('William', [1]),\n",
      " ('Billi', [1]),\n",
      " ('Celine', [1]),\n",
      " ('Woodrow', [1]),\n",
      " ('Sandro', [1]),\n",
      " ('Johannes', [1]),\n",
      " ('Derrek', [1]),\n",
      " ('Kaspar', [1]),\n",
      " ('Darius', [1]),\n",
      " ('Joya', [1]),\n",
      " ('Jessie', [1]),\n",
      " ('Prescott', [1]),\n",
      " ('Carol', [1]),\n",
      " ('Hodge', [1]),\n",
      " ('Winnah', [1]),\n",
      " ('Glad', [1]),\n",
      " ('Juliana', [1]),\n",
      " ('Heath', [1]),\n",
      " ('Aina', [1]),\n",
      " ('Gav', [1]),\n",
      " ('Chrysler', [1]),\n",
      " ('Coral', [1]),\n",
      " ('Darci', [1]),\n",
      " ('Rustin', [1]),\n",
      " ('Evelyn', [1]),\n",
      " ('Rosina', [1]),\n",
      " ('Jessie', [1]),\n",
      " ('Yoko', [1]),\n",
      " ('Angelico', [1]),\n",
      " ('Georges', [1]),\n",
      " ('Claude', [1]),\n",
      " ('Han', [1]),\n",
      " ('Erin', [1]),\n",
      " ('Hervey', [1]),\n",
      " ('Vivienne', [1]),\n",
      " ('Menard', [1]),\n",
      " ('Terrence', [1]),\n",
      " ('Hurley', [1]),\n",
      " ('Finley', [1]),\n",
      " ('Faun', [1]),\n",
      " ('Noam', [1]),\n",
      " ('Ann-Marie', [1]),\n",
      " ('Reta', [1]),\n",
      " ('Teressa', [1]),\n",
      " ('Leslie', [1]),\n",
      " ('Udell', [1]),\n",
      " ('Bertrand', [1]),\n",
      " ('Kara', [1]),\n",
      " ('Rory', [1]),\n",
      " ('Hulda', [1]),\n",
      " ('Anna-Diana', [1]),\n",
      " ('Giff', [1]),\n",
      " ('Dominica', [1]),\n",
      " ('Frederic', [1]),\n",
      " ('Laura', [1]),\n",
      " ('Dasha', [1]),\n",
      " ('Ora', [1]),\n",
      " ('Pauline', [1]),\n",
      " ('Clarita', [1]),\n",
      " ('Andie', [1]),\n",
      " ('Faina', [1]),\n",
      " ('Artur', [1]),\n",
      " ('Ravi', [1]),\n",
      " ('Lynn', [1]),\n",
      " ('Eddie', [1]),\n",
      " ('Auguste', [1]),\n",
      " ('Cate', [1]),\n",
      " ('Laural', [1]),\n",
      " ('Heinrich', [1]),\n",
      " ('Toddy', [1]),\n",
      " ('Nealy', [1]),\n",
      " ('Elihu', [1]),\n",
      " ('Percy', [1]),\n",
      " ('Sargent', [1]),\n",
      " ('Nisse', [1]),\n",
      " ('Layne', [1]),\n",
      " ('Lee', [1]),\n",
      " ('Alex', [1]),\n",
      " ('Redmond', [1]),\n",
      " ('Aimee', [1]),\n",
      " ('Della', [1]),\n",
      " ('August', [1]),\n",
      " ('Blake', [1]),\n",
      " ('Lauri', [1]),\n",
      " ('Cassi', [1]),\n",
      " ('Kaja', [1]),\n",
      " ('Jada', [1]),\n",
      " ('Cristine', [1]),\n",
      " ('Casper', [1]),\n",
      " ('Cappella', [1]),\n",
      " ('Marci', [1]),\n",
      " ('Reiko', [1]),\n",
      " ('Igor', [1]),\n",
      " ('Freddi', [1]),\n",
      " ('Roselyn', [1]),\n",
      " ('Roxane', [1]),\n",
      " ('Blaine', [1]),\n",
      " ('Jared', [1]),\n",
      " ('Donnie', [1]),\n",
      " ('Dionysus', [1]),\n",
      " ('Monroe', [1]),\n",
      " ('Jud', [1]),\n",
      " ('Biff', [1]),\n",
      " ('Hammad', [1]),\n",
      " ('Jude', [1]),\n",
      " ('Tim', [1]),\n",
      " ('Estrellita', [1]),\n",
      " ('Carlotta', [1]),\n",
      " ('Vickie', [1]),\n",
      " ('Jolie', [1]),\n",
      " ('Lucas', [1]),\n",
      " ('Leonard', [1]),\n",
      " ('Raine', [1]),\n",
      " ('Loleta', [1]),\n",
      " ('Antonio', [1]),\n",
      " ('Sully', [1]),\n",
      " ('Elisabeth', [1]),\n",
      " ('Iain', [1]),\n",
      " ('Mendel', [1]),\n",
      " ('Willard', [1]),\n",
      " ('Fredrick', [1]),\n",
      " ('Forest', [1]),\n",
      " ('Geneva', [1]),\n",
      " ('Maribel', [1]),\n",
      " ('Emmie', [1]),\n",
      " ('Inga', [1]),\n",
      " ('Antony', [1]),\n",
      " ('Roslyn', [1]),\n",
      " ('Wally', [1]),\n",
      " ('Masha', [1]),\n",
      " ('Son', [1]),\n",
      " ('Osborne', [1]),\n",
      " ('Cari', [1]),\n",
      " ('Urban', [1]),\n",
      " ('Sven', [1]),\n",
      " ('Bliss', [1]),\n",
      " ('Amory', [1]),\n",
      " ('Bruno', [1]),\n",
      " ('Nevins', [1]),\n",
      " ('Sean', [1]),\n",
      " ('Patsy', [1]),\n",
      " ('Christina', [1]),\n",
      " ('Hilary', [1]),\n",
      " ('Floyd', [1]),\n",
      " ('Wade', [1]),\n",
      " ('Othello', [1]),\n",
      " ('Chen', [1]),\n",
      " ('Clifford', [1]),\n",
      " ('Jefferson', [1]),\n",
      " ('Jaclyn', [1]),\n",
      " ('Albrecht', [1]),\n",
      " ('Herby', [1]),\n",
      " ('Rosalinda', [1]),\n",
      " ('Danie', [1]),\n",
      " ('Daryl', [1]),\n",
      " ('Malena', [1]),\n",
      " ('Tann', [1]),\n",
      " ('Tova', [1]),\n",
      " ('Mei', [1]),\n",
      " ('Lida', [1]),\n",
      " ('Tray', [1]),\n",
      " ('Wright', [1]),\n",
      " ('Cristal', [1]),\n",
      " ('Cesar', [1]),\n",
      " ('Braden', [1]),\n",
      " ('Sylvan', [1]),\n",
      " ('Linus', [1]),\n",
      " ('Theo', [1]),\n",
      " ('Bayard', [1]),\n",
      " ('Trista', [1]),\n",
      " ('Theresa-Marie', [1]),\n",
      " ('Sherman', [1]),\n",
      " ('Forrester', [1]),\n",
      " ('Scottie', [1]),\n",
      " ('Mercy', [1]),\n",
      " ('Flem', [1]),\n",
      " ('Mace', [1]),\n",
      " ('Fraser', [1]),\n",
      " ('Briggs', [1]),\n",
      " ('Barnard', [1]),\n",
      " ('Drea', [1]),\n",
      " ('Mignon', [1]),\n",
      " ('Bengt', [1]),\n",
      " ('Constantin', [1]),\n",
      " ('Edita', [1]),\n",
      " ('Art', [1]),\n",
      " ('Giselle', [1]),\n",
      " ('Matilda', [1]),\n",
      " ('Ritchie', [1]),\n",
      " ('Janis', [1]),\n",
      " ('Bertie', [1]),\n",
      " ('Alexis', [1]),\n",
      " ('Sara', [1]),\n",
      " ('Lily', [1]),\n",
      " ('Ginnie', [1]),\n",
      " ('Cam', [1]),\n",
      " ('Merv', [1]),\n",
      " ('Jonathon', [1]),\n",
      " ('Luisa', [1]),\n",
      " ('Rona', [1]),\n",
      " ('Chan', [1]),\n",
      " ('Shaw', [1]),\n",
      " ('Tierney', [1]),\n",
      " ('Thor', [1]),\n",
      " ('Randal', [1]),\n",
      " ('Woodie', [1]),\n",
      " ('Aurea', [1]),\n",
      " ('Maudie', [1]),\n",
      " ('Merrel', [1]),\n",
      " ('Maggie', [1]),\n",
      " ('Rena', [1]),\n",
      " ('Poppy', [1]),\n",
      " ('Pooh', [1]),\n",
      " ('Riccardo', [1]),\n",
      " ('Karlie', [1]),\n",
      " ('Patin', [1]),\n",
      " ('Brana', [1]),\n",
      " ('Riki', [1]),\n",
      " ('Berk', [1]),\n",
      " ('Amanda', [1]),\n",
      " ('Noreen', [1]),\n",
      " ('Aida', [1]),\n",
      " ('Vallie', [1]),\n",
      " ('Conway', [1]),\n",
      " ('Beret', [1]),\n",
      " ('Jenny', [1]),\n",
      " ('Courtney', [1]),\n",
      " ('Nonna', [1]),\n",
      " ('Vite', [1]),\n",
      " ('Leona', [1]),\n",
      " ('Marcella', [1]),\n",
      " ('Kippy', [1]),\n",
      " ('Mikel', [1]),\n",
      " ('Wallie', [1]),\n",
      " ('Grace', [1]),\n",
      " ('Judith', [1]),\n",
      " ('Oprah', [1]),\n",
      " ('Letta', [1]),\n",
      " ('Kaitlyn', [1]),\n",
      " ('Patty', [1]),\n",
      " ('Derek', [1]),\n",
      " ('Oral', [1]),\n",
      " ('Dawn', [1]),\n",
      " ('Wiley', [1]),\n",
      " ('Windham', [1]),\n",
      " ('Cristi', [1]),\n",
      " ('Susanna', [1]),\n",
      " ('Dayle', [1]),\n",
      " ('Berty', [1]),\n",
      " ('Ursula', [1]),\n",
      " ('Alexander', [1]),\n",
      " ('Nicolle', [1]),\n",
      " ('Elyn', [1]),\n",
      " ('Meredith', [1]),\n",
      " ('Chevalier', [1]),\n",
      " ('Sheela', [1]),\n",
      " ('Neville', [1]),\n",
      " ('Norma', [1]),\n",
      " ('Alston', [1]),\n",
      " ('Candide', [1]),\n",
      " ('Hetty', [1]),\n",
      " ('Shelley', [1]),\n",
      " ('Harriette', [1]),\n",
      " ('Delilah', [1]),\n",
      " ('Nance', [1]),\n",
      " ('Ignacio', [1]),\n",
      " ('Natalia', [1]),\n",
      " ('Philippa', [1]),\n",
      " ('Ekaterina', [1]),\n",
      " ('Bobbie', [1]),\n",
      " ('Arvin', [1]),\n",
      " ('Joselyn', [1]),\n",
      " ('Laureen', [1]),\n",
      " ('Alia', [1]),\n",
      " ('Voltaire', [1]),\n",
      " ('Tate', [1]),\n",
      " ('Natalya', [1]),\n",
      " ('Kenny', [1]),\n",
      " ('Margret', [1]),\n",
      " ('Whit', [1]),\n",
      " ('Lew', [1]),\n",
      " ('Keenan', [1]),\n",
      " ('Jeffry', [1]),\n",
      " ('Maxie', [1]),\n",
      " ('Bee', [1]),\n",
      " ('Pattie', [1]),\n",
      " ('Lolita', [1]),\n",
      " ('Fletch', [1]),\n",
      " ('Jerry', [1]),\n",
      " ('Freida', [1]),\n",
      " ('Lotta', [1]),\n",
      " ('Janelle', [1]),\n",
      " ('Bart', [1]),\n",
      " ('Caspar', [1]),\n",
      " ('Dylan', [1]),\n",
      " ('Burt', [1]),\n",
      " ('Mitchel', [1]),\n",
      " ('Christa', [1]),\n",
      " ('Christie', [1]),\n",
      " ('Myriam', [1]),\n",
      " ('Kee', [1]),\n",
      " ('Antoine', [1]),\n",
      " ('Warner', [1]),\n",
      " ('Florence', [1]),\n",
      " ('Demetri', [1]),\n",
      " ('Brock', [1]),\n",
      " ('Tasha', [1]),\n",
      " ('Mamie', [1]),\n",
      " ('Quinta', [1]),\n",
      " ('Sibley', [1]),\n",
      " ('Marcos', [1]),\n",
      " ('Tarrant', [1]),\n",
      " ('Shlomo', [1]),\n",
      " ('Atlante', [1]),\n",
      " ('Ransom', [1]),\n",
      " ('Davina', [1]),\n",
      " ('Lusa', [1]),\n",
      " ('Chance', [1]),\n",
      " ('Harriet', [1]),\n",
      " ('Corry', [1]),\n",
      " ('Sella', [1]),\n",
      " ('Meryl', [1]),\n",
      " ('Aharon', [1]),\n",
      " ('Roch', [1]),\n",
      " ('Roxana', [1]),\n",
      " ('Maxy', [1]),\n",
      " ('Chelsie', [1]),\n",
      " ('Nevil', [1]),\n",
      " ('Cloe', [1]),\n",
      " ('Heywood', [1]),\n",
      " ('Carlin', [1]),\n",
      " ('Murdock', [1]),\n",
      " ('Gavin', [1]),\n",
      " ('Ronna', [1]),\n",
      " ('Jody', [1]),\n",
      " ('Kip', [1]),\n",
      " ('Gypsy', [1]),\n",
      " ('Aila', [1]),\n",
      " ('Kerri', [1]),\n",
      " ('Hal', [1]),\n",
      " ('Dickey', [1]),\n",
      " ('Manfred', [1]),\n",
      " ('Esta', [1]),\n",
      " ('Tisha', [1]),\n",
      " ('Alta', [1]),\n",
      " ('Ramsey', [1]),\n",
      " ('Gale ', [1]),\n",
      " ('Meryl', [1]),\n",
      " ('Karie', [1]),\n",
      " ('Mortimer', [1]),\n",
      " ('Nickie', [1]),\n",
      " ('Lydia', [1]),\n",
      " ('Belinda', [1]),\n",
      " ('Val', [1]),\n",
      " ('Aaron', [1]),\n",
      " ('Janine', [1]),\n",
      " ('Forester', [1]),\n",
      " ('Teddi', [1]),\n",
      " ('Gerome', [1]),\n",
      " ('Tilda', [1]),\n",
      " ('Pablo', [1]),\n",
      " ('Paola', [1]),\n",
      " ('Mariann', [1]),\n",
      " ('Franny', [1]),\n",
      " ('Trace', [1]),\n",
      " ('Sarge', [1]),\n",
      " ('Somerset', [1]),\n",
      " ('Annemarie', [1]),\n",
      " ('Matteo', [1]),\n",
      " ('Moselle', [1]),\n",
      " ('Harley', [1]),\n",
      " ('Agamemnon', [1]),\n",
      " ('Binky', [1]),\n",
      " ('Kennedy', [1]),\n",
      " ('Fanny', [1]),\n",
      " ('Ileana', [1]),\n",
      " ('Marybeth', [1]),\n",
      " ('Melita', [1]),\n",
      " ('Adeline', [1]),\n",
      " ('Kaylyn', [1]),\n",
      " ('Rhodie', [1]),\n",
      " ('Red', [1]),\n",
      " ('Adair', [1]),\n",
      " ('Hermann', [1]),\n",
      " ('Kari', [1]),\n",
      " ('Chauncey', [1]),\n",
      " ('Kelly', [1]),\n",
      " ('Neale', [1])]\n"
     ]
    }
   ],
   "source": [
    "predictions = []\n",
    "\n",
    "for x_batch, _ in val_dataloader:\n",
    "    p_batch = infer_batch(f, x_batch, device).cpu().numpy().tolist()\n",
    "    predictions.extend(p_batch)\n",
    "    \n",
    "from pprint import pprint\n",
    "pprint(list(zip(names_val, predictions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very convincing! What are these word vectors and how are they helping our network predict baby genders?  ü§î\n",
    "\n",
    "Do the first assignment and find out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:mllanguage] *",
   "language": "python",
   "name": "conda-env-mllanguage-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
